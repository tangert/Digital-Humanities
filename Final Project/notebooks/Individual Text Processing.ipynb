{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from itertools import chain, combinations\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "Getting the data from the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_df = pd.read_csv(\"../data/old-testament-verses.csv\")\n",
    "nt_df = pd.read_csv(\"../data/new-testament-verses.csv\")\n",
    "\n",
    "#rename from AyahText to VerseText\n",
    "q_df = pd.read_csv(\"../data/quran-verses.csv\")\n",
    "q_df.columns = ['DatabaseID', 'SuraID', 'VerseID', 'VerseText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "otv = ot_df[ot_df['VerseText'].str.contains(\"believe\")]\n",
    "ntv = nt_df[nt_df['VerseText'].str.contains(\"believe\")]\n",
    "qv = q_df[q_df['VerseText'].str.contains(\"believe\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines(df):\n",
    "    return list(df[\"VerseText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stops = [w for w in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the stop words the most common words...?\n",
    "new_stops = ['hath', \"'s\",'an', 'let','behold', 'went','o','hast','thine','like','thing','things','quot','and', 'in', 'thou', 'thee', 'thy', 'unto', 'ye', 'said', 'saith', 'shall', 'shalt', 'yea', 'thereof']\n",
    "all_stops += new_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "regex = re.compile('[%s]' % re.escape(punctuation))\n",
    "\n",
    "def strip_punc(s):  # From Vinko's solution, with fix.\n",
    "    return regex.sub('', s)\n",
    "\n",
    "def clean_text(text):\n",
    "    # basic nlp clean up\n",
    "    lm = WordNetLemmatizer()\n",
    "    st = SnowballStemmer(\"english\")\n",
    "\n",
    "    base = [strip_punc(t).lower() for t in word_tokenize(text)\n",
    "            if t not in punctuation\n",
    "            and t.lower() not in all_stops]\n",
    "    \n",
    "    lemmatized = [lm.lemmatize(w) for w in base]\n",
    "#     stemmed = [st.stem(w) for w in lemmatized]\n",
    "    no_chars = [w for w in lemmatized if len(w) > 1]\n",
    "    \n",
    "    return no_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab preparation\n",
    "\n",
    "Constructing a holistic Doc2Vec model for all of the texts put together. Basically, this is just Word2Vec but tagging each word with which religion it belongs to (according to a probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(df):\n",
    "    vocab = []\n",
    "    for l in get_lines(df):\n",
    "        tokens = tokenize(l)\n",
    "        vocab += tokens\n",
    "    return set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shared(sets):\n",
    "    # Finds the intersection of all the input sets\n",
    "    return reduce((lambda set1,set2: set1&set2), sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sym_diffs(sets):\n",
    "    # Finds the symm etric difference of a list of sets\n",
    "    return reduce((lambda set1, set2: set1.symmetric_difference(set2)), sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_uniques(sets):\n",
    "    # Gets the unique elements in each set\n",
    "    # return a dictionary with labels of each\n",
    "    intersection = get_shared(sets)\n",
    "    sym_diffs = get_sym_diffs(sets)\n",
    "    return sym_diffs - intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_vocabs(vocabs_dict):\n",
    "    all_uniques = get_all_uniques(vocabs_dict.values())\n",
    "    unique_dict = {}\n",
    "    \n",
    "    for tag, vocab in vocabs_dict.items():\n",
    "        unique_dict[tag] = []\n",
    "        for w in vocab:\n",
    "            if w in all_uniques:\n",
    "                unique_dict[tag].append(w)\n",
    "    \n",
    "    return unique_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(vocab, tag):\n",
    "    d = {}\n",
    "    for w in vocab:\n",
    "        d[w] = tag\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(dicts): \n",
    "    super_dict = {}\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            super_dict[k] = v\n",
    "    return super_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def have_common(l1,l2):\n",
    "    result = False\n",
    "    for x in l1: \n",
    "        for y in l2: \n",
    "            if x == y:\n",
    "                print(\"Got same:\", x, y)\n",
    "                result = True\n",
    "                return result  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Word2Vec attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(l):\n",
    "    lmin = min(l)\n",
    "    lmax = max(l)\n",
    "    return [(v-lmin)/(lmax-lmin) for v in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(d, rev):\n",
    "    return sorted(d.items(), key=lambda kv: kv[1], reverse=rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_docs(df):\n",
    "    verses = list(df[\"VerseText\"])\n",
    "    docs = [clean_text(v) for v in verses]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_origin(model, origin_word):\n",
    "    \n",
    "    # gets the original vector for the chosen origin word\n",
    "    origin_vec = model.wv.get_vector(origin_word)\n",
    "    \n",
    "    # used to calculate the new origin\n",
    "    zero_vec = np.zeros(origin_vec.shape)\n",
    "    \n",
    "    # vector to shift each point by everything by\n",
    "    transformation_vec = zero_vec - origin_vec\n",
    "    \n",
    "    # dict to store all the new vectors\n",
    "    transformed_vecs = {}\n",
    "    \n",
    "    for w in model.wv.vocab:\n",
    "        # original vector for the word\n",
    "        w_vec = model.wv.get_vector(w)\n",
    "        \n",
    "        # shifted by the transformation\n",
    "        transformed_w_vec = w_vec + transformation_vec\n",
    "        \n",
    "        # store\n",
    "        transformed_vecs[w] = transformed_w_vec\n",
    "        \n",
    "    return transformed_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the documents from each text.\n",
    "ot_docs = get_word2vec_docs(ot_df)\n",
    "nt_docs = get_word2vec_docs(nt_df)\n",
    "q_docs = get_word2vec_docs(q_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global hyper parameters\n",
    "hp = {\n",
    "    \"size\": 150, # size of the one-hot-encoded word vectors\n",
    "    \"window\": 5, # context size\n",
    "    \"min_count\": 2,\n",
    "    \"workers\": 4,\n",
    "    \"iter\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(docs):\n",
    "    model = gensim.models.Word2Vec(\n",
    "        docs,\n",
    "        size=hp[\"size\"],\n",
    "        window=hp[\"window\"],\n",
    "        min_count=hp[\"min_count\"],\n",
    "        workers=hp[\"workers\"])\n",
    "    \n",
    "    # initialize similarities\n",
    "    model.train(docs, total_examples=len(docs), epochs=50)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning models to compare similar words across religions\n",
    "\n",
    "Note that for the new Gensim versions, calls for .index2word, .vocab, .syn0 and .syn0norm should be replaced with .wv.index2word, .wv.vocab, .wv.syn0 and .wv.syn0norm respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_models(models, words=None):\n",
    "    \"\"\"\n",
    "    Intersect any number of gensim models.\n",
    "    Generalized from original two-way intersection.\n",
    "    \n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocabs = [set(m.wv.vocab.keys()) for m in models]\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = reduce((lambda vocab1,vocab2: vocab1&vocab2), vocabs)\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    \n",
    "    # This was generalized from:\n",
    "    # if not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
    "    #   return (m1,m2)\n",
    "    if all(not vocab-common_vocab for vocab in vocabs):\n",
    "        print(\"All identical!\")\n",
    "        return models\n",
    "        \n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: sum([m.wv.vocab[w].count for m in models]),reverse=True)\n",
    "    \n",
    "    # Then for each model...\n",
    "    for m in models:\n",
    "        \n",
    "        # Replace old vectors_norm array with new one (with common vocab)\n",
    "        indices = [m.wv.vocab[w].index for w in common_vocab]\n",
    "                \n",
    "        old_arr = m.wv.vectors_norm\n",
    "                \n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.vectors_norm = m.wv.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.wv.index2word = common_vocab\n",
    "        old_vocab = m.wv.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.wv.vocab = new_vocab\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExample usage:\\nmodel1 = [a gensim model I have for text published in the 1750s]\\nmodel2 = [a gensim model I have for text published in the 1850s]\\n# The word \\'god\\' does not change much in meaning:\\n    In [61]: measure_semantic_shift_by_neighborhood(model1,model2,\\'god\\',k=10,verbose=True)\\n    \\n    >> Neighborhood of associations of the word \"god\" in model1:\\n    almighty, jehovah, creator, uncreated, omniscient, logos, righteousness, christ, redeemer, salvation\\n    >> Neighborhood of associations of the word \"god\" in model2:\\n    almighty, heaven, jehovah, creator, redeemer, christ, divine, righteousness, providence, saviour\\n    \\n    Out[61]: 0.011609088245951749\\n# The word \\'matter\\' does, moving from meaning mainly the \"matter\" of the universe to \"what is the matter\":\\n    In [62]: measure_semantic_shift_by_neighborhood(model1,model2,\\'matter\\',k=10,verbose=True)\\n    \\n    >> Neighborhood of associations of the word \"matter\" in model1:\\n    cohesion, sediment, menstruum, purulent, conceivable, gelatinous, morbific, compression, cerebellum, divisible\\n    >> Neighborhood of associations of the word \"matter\" in model2:\\n    matters, question, subject, affair, substance, concernment, concerns, questions, controversy, discussion\\n    \\n    Out[62]: 0.0847526073498025\\n# The word \\'station\\' changes even more, moving from meaning one\\'s social rank or \"station\", to a train station:\\n    In [63]: measure_semantic_shift_by_neighborhood(model1,model2,\\'station\\',k=10,verbose=True)\\n    \\n    >> Neighborhood of associations of the word \"station\" in model1:\\n    stations, dation, sphere, employments, deg, vocation, personate, lowest, district, apprenticeship\\n    >> Neighborhood of associations of the word \"station\" in model2:\\n    stations, train, posts, position, situation, town, carriage, stationed, rank, cab\\n    \\n    Out[63]: 0.14173381265358098\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def measure_semantic_shift(model1,model2,word,k=25,verbose=False):\n",
    "    \"\"\"\n",
    "    Basic implementation of William Hamilton (@williamleif) et al's measure of semantic change\n",
    "    proposed in their paper \"Cultural Shift or Linguistic Drift?\" (https://arxiv.org/abs/1606.02821),\n",
    "    which they call the \"local neighborhood measure.\" They find this measure better suited to understand\n",
    "    the semantic change of nouns owing to \"cultural shift,\" or changes in meaning \"local\" to that word,\n",
    "    rather than global changes in language (\"linguistic drift\") use that are better suited to a\n",
    "    Procrustes-alignment method (also described in the same paper.)\n",
    "    \n",
    "    Arguments are:\n",
    "    - `model1`, `model2`: Are gensim word2vec models.\n",
    "    - `word` is a sting representation of a given word.\n",
    "    - `k` is the size of the word's neighborhood (# of its closest words in its vector space).\n",
    "    \"\"\"\n",
    "    # Import function for cosine distance\n",
    "    from scipy.spatial.distance import cosine\n",
    "    \n",
    "    # Check that this word is present in both models\n",
    "    if not word in model1.wv.vocab or not word in model2.wv.vocab:\n",
    "        print(\"!! Word %s not present in both models.\" % word)\n",
    "        return None\n",
    "    \n",
    "    # Get the two neighborhoods\n",
    "    neighborhood1 = [w for w,c in model1.most_similar(word,topn=k)]\n",
    "    neighborhood2 = [w for w,c in model2.most_similar(word,topn=k)]\n",
    "    \n",
    "    # Print?\n",
    "    if verbose:\n",
    "        print('>> Neighborhood of associations of the word \"%s\" in model1:' % word)\n",
    "        print(', '.join(neighborhood1))\n",
    "        print('>> Neighborhood of associations of the word \"%s\" in model2:' % word)\n",
    "        print(', '.join(neighborhood2))\n",
    "    \n",
    "    # Get the 'meta' neighborhood (both combined)\n",
    "    meta_neighborhood = list(set(neighborhood1)|set(neighborhood2))\n",
    "    \n",
    "    # Filter the meta neighborhood so that it contains only words present in both models\n",
    "    meta_neighborhood = [w for w in meta_neighborhood if w in model1.wv.vocab and w in model2.wv.vocab]\n",
    "    \n",
    "    # For both models, get a similarity vector between the focus word and all of the words in the meta neighborhood\n",
    "    vector1 = [model1.similarity(word,w) for w in meta_neighborhood]\n",
    "    vector2 = [model2.similarity(word,w) for w in meta_neighborhood]\n",
    "    \n",
    "    # Compute the cosine distance *between* those similarity vectors\n",
    "    dist=cosine(vector1,vector2)\n",
    "    \n",
    "    # Return this cosine distance -- a measure of the relative semantic shift for this word between these two models\n",
    "    return dist\n",
    "    \n",
    "\"\"\"\n",
    "Example usage:\n",
    "model1 = [a gensim model I have for text published in the 1750s]\n",
    "model2 = [a gensim model I have for text published in the 1850s]\n",
    "# The word 'god' does not change much in meaning:\n",
    "    In [61]: measure_semantic_shift_by_neighborhood(model1,model2,'god',k=10,verbose=True)\n",
    "    \n",
    "    >> Neighborhood of associations of the word \"god\" in model1:\n",
    "    almighty, jehovah, creator, uncreated, omniscient, logos, righteousness, christ, redeemer, salvation\n",
    "    >> Neighborhood of associations of the word \"god\" in model2:\n",
    "    almighty, heaven, jehovah, creator, redeemer, christ, divine, righteousness, providence, saviour\n",
    "    \n",
    "    Out[61]: 0.011609088245951749\n",
    "# The word 'matter' does, moving from meaning mainly the \"matter\" of the universe to \"what is the matter\":\n",
    "    In [62]: measure_semantic_shift_by_neighborhood(model1,model2,'matter',k=10,verbose=True)\n",
    "    \n",
    "    >> Neighborhood of associations of the word \"matter\" in model1:\n",
    "    cohesion, sediment, menstruum, purulent, conceivable, gelatinous, morbific, compression, cerebellum, divisible\n",
    "    >> Neighborhood of associations of the word \"matter\" in model2:\n",
    "    matters, question, subject, affair, substance, concernment, concerns, questions, controversy, discussion\n",
    "    \n",
    "    Out[62]: 0.0847526073498025\n",
    "# The word 'station' changes even more, moving from meaning one's social rank or \"station\", to a train station:\n",
    "    In [63]: measure_semantic_shift_by_neighborhood(model1,model2,'station',k=10,verbose=True)\n",
    "    \n",
    "    >> Neighborhood of associations of the word \"station\" in model1:\n",
    "    stations, dation, sphere, employments, deg, vocation, personate, lowest, district, apprenticeship\n",
    "    >> Neighborhood of associations of the word \"station\" in model2:\n",
    "    stations, train, posts, position, situation, town, carriage, stationed, rank, cab\n",
    "    \n",
    "    Out[63]: 0.14173381265358098\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train the models on each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO : PROGRESS: at sentence #10000, processed 97904 words, keeping 5861 word types\n",
      "INFO : PROGRESS: at sentence #20000, processed 182562 words, keeping 8873 word types\n",
      "INFO : collected 9377 word types from a corpus of 214613 raw words and 23145 sentences\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : min_count=2 retains 6304 unique words (67% of original 9377, drops 3073)\n",
      "INFO : min_count=2 leaves 211540 word corpus (98% of original 214613, drops 3073)\n",
      "INFO : deleting the raw counts dictionary of 9377 items\n",
      "INFO : sample=0.001 downsamples 44 most-common words\n",
      "INFO : downsampling leaves estimated 184285 word corpus (87.1% of prior 211540)\n",
      "INFO : estimated required memory for 6304 words and 150 dimensions: 10716800 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : training model with 4 workers on 6304 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 214613 raw words (184183 effective words) took 0.1s, 2198099 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 214613 raw words (184343 effective words) took 0.1s, 1638441 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 214613 raw words (184194 effective words) took 0.1s, 1672873 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 214613 raw words (184203 effective words) took 0.1s, 1826795 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 214613 raw words (184401 effective words) took 0.1s, 1962227 effective words/s\n",
      "INFO : training on a 1073065 raw words (921324 effective words) took 0.6s, 1669487 effective words/s\n",
      "WARNING : Effective 'alpha' higher than previous training cycles\n",
      "INFO : training model with 4 workers on 6304 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 214613 raw words (184223 effective words) took 0.1s, 1773478 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 214613 raw words (184279 effective words) took 0.1s, 1760197 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 214613 raw words (184217 effective words) took 0.1s, 1737167 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 214613 raw words (184277 effective words) took 0.1s, 1741488 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 214613 raw words (184394 effective words) took 0.1s, 1791060 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 6 : training on 214613 raw words (184324 effective words) took 0.1s, 1955683 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 7 : training on 214613 raw words (184337 effective words) took 0.1s, 1933452 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 8 : training on 214613 raw words (184096 effective words) took 0.1s, 1938462 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 9 : training on 214613 raw words (184247 effective words) took 0.1s, 1933026 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 10 : training on 214613 raw words (184209 effective words) took 0.1s, 2008201 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 11 : training on 214613 raw words (184232 effective words) took 0.1s, 1974317 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 12 : training on 214613 raw words (184376 effective words) took 0.1s, 1958696 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 13 : training on 214613 raw words (184406 effective words) took 0.1s, 2006798 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 14 : training on 214613 raw words (184306 effective words) took 0.1s, 1806085 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 15 : training on 214613 raw words (184063 effective words) took 0.1s, 1993109 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 16 : training on 214613 raw words (184245 effective words) took 0.1s, 1838086 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 17 : training on 214613 raw words (184219 effective words) took 0.1s, 1363318 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 18 : training on 214613 raw words (184266 effective words) took 0.1s, 1548607 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 19 : training on 214613 raw words (184288 effective words) took 0.1s, 1903646 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 20 : training on 214613 raw words (184298 effective words) took 0.1s, 2008107 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 21 : training on 214613 raw words (184297 effective words) took 0.1s, 1998629 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 22 : training on 214613 raw words (184434 effective words) took 0.1s, 1802111 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 23 : training on 214613 raw words (184260 effective words) took 0.1s, 1647856 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 24 : training on 214613 raw words (184285 effective words) took 0.1s, 1696078 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 25 : training on 214613 raw words (184166 effective words) took 0.1s, 1686525 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 26 : training on 214613 raw words (184607 effective words) took 0.1s, 1969770 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 27 : training on 214613 raw words (184467 effective words) took 0.1s, 1801253 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 28 : training on 214613 raw words (184178 effective words) took 0.1s, 1789542 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 29 : training on 214613 raw words (184128 effective words) took 0.1s, 1723725 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 30 : training on 214613 raw words (184206 effective words) took 0.1s, 1794937 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 31 : training on 214613 raw words (184180 effective words) took 0.1s, 1880981 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 32 : training on 214613 raw words (184331 effective words) took 0.1s, 1773610 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 33 : training on 214613 raw words (184256 effective words) took 0.1s, 1758460 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 34 : training on 214613 raw words (184056 effective words) took 0.1s, 1726276 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 35 : training on 214613 raw words (184141 effective words) took 0.1s, 1821777 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : EPOCH - 36 : training on 214613 raw words (184242 effective words) took 0.1s, 1659943 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 37 : training on 214613 raw words (184155 effective words) took 0.1s, 1728176 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 38 : training on 214613 raw words (184194 effective words) took 0.1s, 2045611 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 39 : training on 214613 raw words (184278 effective words) took 0.1s, 1910599 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 40 : training on 214613 raw words (184451 effective words) took 0.1s, 1954139 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 41 : training on 214613 raw words (184262 effective words) took 0.1s, 1991505 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 42 : training on 214613 raw words (184301 effective words) took 0.1s, 1823592 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 43 : training on 214613 raw words (184301 effective words) took 0.1s, 1807025 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 44 : training on 214613 raw words (184076 effective words) took 0.1s, 1896896 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 45 : training on 214613 raw words (184226 effective words) took 0.1s, 2036370 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 46 : training on 214613 raw words (184128 effective words) took 0.1s, 1745945 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 47 : training on 214613 raw words (184362 effective words) took 0.1s, 1662350 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 48 : training on 214613 raw words (184433 effective words) took 0.1s, 1789338 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 49 : training on 214613 raw words (184205 effective words) took 0.1s, 2051913 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 50 : training on 214613 raw words (184152 effective words) took 0.1s, 2060203 effective words/s\n",
      "INFO : training on a 10730650 raw words (9213060 effective words) took 5.5s, 1671959 effective words/s\n",
      "INFO : precomputing L2-norms of word weight vectors\n",
      "WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO : collected 5152 word types from a corpus of 60551 raw words and 7957 sentences\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : min_count=2 retains 3417 unique words (66% of original 5152, drops 1735)\n",
      "INFO : min_count=2 leaves 58816 word corpus (97% of original 60551, drops 1735)\n",
      "INFO : deleting the raw counts dictionary of 5152 items\n",
      "INFO : sample=0.001 downsamples 59 most-common words\n",
      "INFO : downsampling leaves estimated 52063 word corpus (88.5% of prior 58816)\n",
      "INFO : estimated required memory for 3417 words and 150 dimensions: 5808900 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : training model with 4 workers on 3417 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 60551 raw words (52111 effective words) took 0.0s, 1603321 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 60551 raw words (52098 effective words) took 0.0s, 1306702 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 60551 raw words (51997 effective words) took 0.0s, 1205478 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 60551 raw words (52119 effective words) took 0.0s, 1451465 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 60551 raw words (52012 effective words) took 0.0s, 1471388 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : training on a 302755 raw words (260337 effective words) took 0.2s, 1122825 effective words/s\n",
      "WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING : Effective 'alpha' higher than previous training cycles\n",
      "INFO : training model with 4 workers on 3417 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 60551 raw words (52125 effective words) took 0.0s, 1320550 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 60551 raw words (52011 effective words) took 0.0s, 1454377 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 60551 raw words (51963 effective words) took 0.0s, 1437395 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 60551 raw words (52079 effective words) took 0.0s, 1451856 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 60551 raw words (52028 effective words) took 0.0s, 1449176 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 6 : training on 60551 raw words (52061 effective words) took 0.0s, 1671628 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 7 : training on 60551 raw words (52080 effective words) took 0.0s, 1350594 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 8 : training on 60551 raw words (52034 effective words) took 0.0s, 1158126 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 9 : training on 60551 raw words (52040 effective words) took 0.0s, 1240846 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 10 : training on 60551 raw words (52021 effective words) took 0.0s, 1398217 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 11 : training on 60551 raw words (52092 effective words) took 0.0s, 1577269 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 12 : training on 60551 raw words (52046 effective words) took 0.0s, 1578848 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 13 : training on 60551 raw words (52060 effective words) took 0.0s, 1497824 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 14 : training on 60551 raw words (52095 effective words) took 0.0s, 1541924 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 15 : training on 60551 raw words (51997 effective words) took 0.0s, 1502979 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 16 : training on 60551 raw words (52101 effective words) took 0.0s, 1495993 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 17 : training on 60551 raw words (52042 effective words) took 0.0s, 1451974 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 18 : training on 60551 raw words (52100 effective words) took 0.0s, 1463010 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 19 : training on 60551 raw words (52064 effective words) took 0.0s, 1409808 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 20 : training on 60551 raw words (52083 effective words) took 0.0s, 1480097 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 21 : training on 60551 raw words (52071 effective words) took 0.0s, 1527022 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 22 : training on 60551 raw words (52022 effective words) took 0.0s, 1518385 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 23 : training on 60551 raw words (52222 effective words) took 0.0s, 1453532 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 24 : training on 60551 raw words (52026 effective words) took 0.0s, 1480947 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 25 : training on 60551 raw words (52122 effective words) took 0.0s, 1521842 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 26 : training on 60551 raw words (51979 effective words) took 0.0s, 1465986 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 27 : training on 60551 raw words (51917 effective words) took 0.0s, 1532178 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 28 : training on 60551 raw words (52135 effective words) took 0.0s, 1595528 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 29 : training on 60551 raw words (51995 effective words) took 0.0s, 1478383 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 30 : training on 60551 raw words (52133 effective words) took 0.0s, 1475970 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 31 : training on 60551 raw words (52028 effective words) took 0.0s, 1560998 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 32 : training on 60551 raw words (52149 effective words) took 0.0s, 1466147 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 33 : training on 60551 raw words (52068 effective words) took 0.0s, 1569464 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 34 : training on 60551 raw words (52061 effective words) took 0.0s, 1513306 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 35 : training on 60551 raw words (52096 effective words) took 0.0s, 1546080 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 36 : training on 60551 raw words (52049 effective words) took 0.0s, 1459988 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 37 : training on 60551 raw words (52053 effective words) took 0.0s, 1541489 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 38 : training on 60551 raw words (52061 effective words) took 0.0s, 1575174 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 39 : training on 60551 raw words (52093 effective words) took 0.0s, 1359227 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 40 : training on 60551 raw words (52120 effective words) took 0.0s, 1304374 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 41 : training on 60551 raw words (52095 effective words) took 0.0s, 1310891 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 42 : training on 60551 raw words (52028 effective words) took 0.0s, 1459403 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 43 : training on 60551 raw words (52067 effective words) took 0.0s, 1495622 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 44 : training on 60551 raw words (52023 effective words) took 0.0s, 1491130 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 45 : training on 60551 raw words (52036 effective words) took 0.0s, 1538246 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 46 : training on 60551 raw words (52183 effective words) took 0.0s, 1482262 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 47 : training on 60551 raw words (52100 effective words) took 0.0s, 1445954 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 48 : training on 60551 raw words (52039 effective words) took 0.0s, 1525442 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 49 : training on 60551 raw words (52009 effective words) took 0.0s, 1551330 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 50 : training on 60551 raw words (52009 effective words) took 0.0s, 1544141 effective words/s\n",
      "INFO : training on a 3027550 raw words (2603111 effective words) took 2.3s, 1150437 effective words/s\n",
      "INFO : precomputing L2-norms of word weight vectors\n",
      "WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO : collected 5703 word types from a corpus of 56975 raw words and 6236 sentences\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : min_count=2 retains 3466 unique words (60% of original 5703, drops 2237)\n",
      "INFO : min_count=2 leaves 54738 word corpus (96% of original 56975, drops 2237)\n",
      "INFO : deleting the raw counts dictionary of 5703 items\n",
      "INFO : sample=0.001 downsamples 45 most-common words\n",
      "INFO : downsampling leaves estimated 48112 word corpus (87.9% of prior 54738)\n",
      "INFO : estimated required memory for 3466 words and 150 dimensions: 5892200 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : training model with 4 workers on 3466 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 56975 raw words (48054 effective words) took 0.0s, 1582195 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 56975 raw words (48084 effective words) took 0.0s, 1475115 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 56975 raw words (48134 effective words) took 0.0s, 1532529 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 56975 raw words (48028 effective words) took 0.0s, 1342657 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 56975 raw words (48135 effective words) took 0.0s, 1320377 effective words/s\n",
      "INFO : training on a 284875 raw words (240435 effective words) took 0.2s, 1165465 effective words/s\n",
      "WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING : Effective 'alpha' higher than previous training cycles\n",
      "INFO : training model with 4 workers on 3466 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 56975 raw words (48189 effective words) took 0.0s, 1577089 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 56975 raw words (48142 effective words) took 0.0s, 1385884 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 56975 raw words (48105 effective words) took 0.0s, 1523552 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 56975 raw words (48210 effective words) took 0.0s, 1623395 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 56975 raw words (48075 effective words) took 0.0s, 1528308 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 6 : training on 56975 raw words (48133 effective words) took 0.0s, 1619413 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 7 : training on 56975 raw words (48078 effective words) took 0.0s, 1614551 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 8 : training on 56975 raw words (48077 effective words) took 0.0s, 1500407 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 9 : training on 56975 raw words (48080 effective words) took 0.0s, 1459367 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 10 : training on 56975 raw words (48177 effective words) took 0.0s, 1608010 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 11 : training on 56975 raw words (48095 effective words) took 0.0s, 1635972 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 12 : training on 56975 raw words (48113 effective words) took 0.0s, 1539601 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 13 : training on 56975 raw words (48090 effective words) took 0.0s, 1599891 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 14 : training on 56975 raw words (48072 effective words) took 0.0s, 1614978 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 15 : training on 56975 raw words (48100 effective words) took 0.0s, 1435628 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 16 : training on 56975 raw words (48189 effective words) took 0.0s, 1741487 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 17 : training on 56975 raw words (48131 effective words) took 0.0s, 1564538 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 18 : training on 56975 raw words (48115 effective words) took 0.0s, 1517429 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 19 : training on 56975 raw words (48185 effective words) took 0.0s, 1498434 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 20 : training on 56975 raw words (48144 effective words) took 0.0s, 1493023 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 21 : training on 56975 raw words (48182 effective words) took 0.0s, 1478464 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 22 : training on 56975 raw words (48087 effective words) took 0.0s, 1433019 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 23 : training on 56975 raw words (48132 effective words) took 0.0s, 1514634 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 24 : training on 56975 raw words (48071 effective words) took 0.0s, 1597368 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 25 : training on 56975 raw words (48155 effective words) took 0.0s, 1618655 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 26 : training on 56975 raw words (48036 effective words) took 0.0s, 1508441 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 27 : training on 56975 raw words (48092 effective words) took 0.0s, 1568645 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 28 : training on 56975 raw words (48210 effective words) took 0.0s, 1595403 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 29 : training on 56975 raw words (48075 effective words) took 0.0s, 1714713 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 30 : training on 56975 raw words (48116 effective words) took 0.0s, 1585680 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 31 : training on 56975 raw words (48003 effective words) took 0.0s, 1530616 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 32 : training on 56975 raw words (47972 effective words) took 0.0s, 1636036 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 33 : training on 56975 raw words (48071 effective words) took 0.0s, 1564578 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 34 : training on 56975 raw words (48140 effective words) took 0.0s, 1442008 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 35 : training on 56975 raw words (48037 effective words) took 0.0s, 1583274 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 36 : training on 56975 raw words (48049 effective words) took 0.0s, 1534176 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 37 : training on 56975 raw words (48136 effective words) took 0.0s, 1539969 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 38 : training on 56975 raw words (48098 effective words) took 0.0s, 1540914 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 39 : training on 56975 raw words (48095 effective words) took 0.0s, 1576666 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 40 : training on 56975 raw words (48090 effective words) took 0.0s, 1511578 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 41 : training on 56975 raw words (48143 effective words) took 0.0s, 1502186 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 42 : training on 56975 raw words (48199 effective words) took 0.0s, 1407578 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 43 : training on 56975 raw words (48193 effective words) took 0.0s, 1516390 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 44 : training on 56975 raw words (48063 effective words) took 0.0s, 1510411 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 45 : training on 56975 raw words (48155 effective words) took 0.0s, 1559229 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 46 : training on 56975 raw words (48037 effective words) took 0.0s, 1777191 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 47 : training on 56975 raw words (48153 effective words) took 0.0s, 1602746 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 48 : training on 56975 raw words (48110 effective words) took 0.0s, 1609445 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 49 : training on 56975 raw words (48152 effective words) took 0.0s, 1513093 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 50 : training on 56975 raw words (48031 effective words) took 0.0s, 1507934 effective words/s\n",
      "INFO : training on a 2848750 raw words (2405583 effective words) took 1.9s, 1236413 effective words/s\n",
      "INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "ot_model = get_model(ot_docs)\n",
    "ot_model.init_sims()\n",
    "\n",
    "nt_model = get_model(nt_docs)\n",
    "nt_model.init_sims()\n",
    "\n",
    "q_model = get_model(q_docs)\n",
    "q_model.init_sims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
     ]
    }
   ],
   "source": [
    "# INTERSECT THE MODELS ON THE COMMON VOCABULARY AND ALIGN.\n",
    "intersected_model = align_models([ot_model,nt_model,q_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REASSIGN NEW INTERSECTED MODELS\n",
    "ot_model = intersected_model[0]\n",
    "nt_model = intersected_model[1]\n",
    "q_model = intersected_model[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('revealed', 0.5445148944854736),\n",
       " ('declared', 0.5360902547836304),\n",
       " ('understood', 0.5340793132781982),\n",
       " ('understand', 0.497624009847641),\n",
       " ('wonder', 0.4894428849220276),\n",
       " ('talk', 0.46386152505874634),\n",
       " ('deceive', 0.4562837481498718),\n",
       " ('dream', 0.45255041122436523),\n",
       " ('declare', 0.44632279872894287),\n",
       " ('sorcerer', 0.4412204921245575)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ot_model.most_similar(positive=['believe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('plainly', 0.600974440574646),\n",
       " ('doubt', 0.5910286903381348),\n",
       " ('behaved', 0.5832509398460388),\n",
       " ('glorified', 0.4768655300140381),\n",
       " ('vengeance', 0.47010985016822815),\n",
       " ('seek', 0.4594820439815521),\n",
       " ('testify', 0.45367687940597534),\n",
       " ('salvation', 0.4528679847717285),\n",
       " ('marvel', 0.4439432621002197),\n",
       " ('try', 0.4341239929199219)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt_model.most_similar(positive=['believe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('prevail', 0.5654722452163696),\n",
       " ('faith', 0.5633742213249207),\n",
       " ('believeth', 0.5428281426429749),\n",
       " ('altogether', 0.5104032754898071),\n",
       " ('revealed', 0.5058085322380066),\n",
       " ('reckoned', 0.5042601823806763),\n",
       " ('sell', 0.4980141818523407),\n",
       " ('hearken', 0.4923272728919983),\n",
       " ('obey', 0.4836070239543915),\n",
       " ('righteousness', 0.47314804792404175)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.most_similar(positive=['believe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Neighborhood of associations of the word \"light\" in model1:\n",
      "darkness, shining, lamp, path, dark, brightness, night, cloud, star, walking, step, prayer, shine, countenance, clear, moon, withdraw, lightning, sun, blind, walk, spot, shadow, understand, lighten\n",
      ">> Neighborhood of associations of the word \"light\" in model2:\n",
      "darkness, shine, lighten, shadow, shining, sun, dark, moon, brightness, chamber, pain, heat, hidden, lack, walk, folly, burning, aware, suddenly, blindness, glorious, giveth, burned, lower, fourth\n",
      ">> Neighborhood of associations of the word \"light\" in model1:\n",
      "darkness, shine, lighten, shadow, shining, sun, dark, moon, brightness, chamber, pain, heat, hidden, lack, walk, folly, burning, aware, suddenly, blindness, glorious, giveth, burned, lower, fourth\n",
      ">> Neighborhood of associations of the word \"light\" in model2:\n",
      "darkness, depth, lamp, walk, glass, glorious, star, moon, leg, esteem, run, lead, leaveth, guide, sun, astray, equal, blind, leaf, shining, pleasing, book, judged, straight, swim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    }
   ],
   "source": [
    "# comparing old testament to new testament, then new testament to quran because of the chronology of the religions\n",
    "word = 'light'\n",
    "ot_to_nt = measure_semantic_shift(ot_model,nt_model,word,k=25,verbose=True)\n",
    "nt_to_q = measure_semantic_shift(nt_model,q_model,word,k=25,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.270130335481935, 0.2840591278427892)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ot_to_nt, nt_to_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "ot_X = ot_model[ot_model.wv.vocab]\n",
    "ot_reduced = pca.fit_transform(ot_X)\n",
    "\n",
    "nt_X = nt_model[nt_model.wv.vocab]\n",
    "nt_reduced = pca.fit_transform(nt_X)\n",
    "\n",
    "q_X = q_model[q_model.wv.vocab]\n",
    "q_reduced = pca.fit_transform(q_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_dict = {}\n",
    "wv_dict[\"ot\"] = {}\n",
    "wv_dict[\"nt\"] = {}\n",
    "wv_dict[\"q\"] = {}\n",
    "\n",
    "for i, word in enumerate(list(ot_model.wv.vocab)):\n",
    "    wv_dict[\"ot\"][word] = [float(ot_reduced[i][0]),float(ot_reduced[i][1])]\n",
    "    \n",
    "for i, word in enumerate(list(nt_model.wv.vocab)):\n",
    "    wv_dict[\"nt\"][word] = [float(nt_reduced[i][0]),float(nt_reduced[i][1])]\n",
    "    \n",
    "for i, word in enumerate(list(q_model.wv.vocab)):\n",
    "    wv_dict[\"q\"][word] = [float(q_reduced[i][0]),float(q_reduced[i][1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon construction from Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import geometry as g\n",
    "from dxfwrite import DXFEngine as dxf\n",
    "from dxfwrite.const import CENTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['faith','sight', 'birth', 'prophet', 'prophecy', 'trust', 'remember', 'experience', 'dream', 'god', 'heaven', 'hell', 'love', 'hate', 'free', 'vision', 'believe',  'light', 'forget', 'darkness', 'peace', 'war', 'life', 'death', 'man', 'woman', 'child', 'eat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    }
   ],
   "source": [
    "topic_vecs = {}\n",
    "\n",
    "for t in topics:\n",
    "    topic_vecs[t] = {}\n",
    "\n",
    "    # go through each religion\n",
    "    for rel in wv_dict:\n",
    "        \n",
    "        model = None\n",
    "        if rel == 'ot':\n",
    "            model = ot_model\n",
    "        elif rel == 'nt':\n",
    "            model = nt_model\n",
    "        elif rel == 'q':\n",
    "            model = q_model\n",
    "            \n",
    "        most_sim = model.most_similar(positive=[t], topn=5)\n",
    "        topic_vecs[t][rel] = {\n",
    "            \"vec\": wv_dict[rel][t],\n",
    "            \"sim\": {}\n",
    "        }\n",
    "        \n",
    "        for w in most_sim:\n",
    "            wv = wv_dict[rel][w[0]]\n",
    "            topic_vecs[t][rel][\"sim\"][w[0]] = wv_dict[rel][w[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ot': {'vec': [-0.19729824364185333, 0.0063003734685480595],\n",
       "  'sim': {'interpretation': [-0.30307742953300476, -0.04603069648146629],\n",
       "   'doubt': [-0.0785498097538948, 0.13480640947818756],\n",
       "   'vision': [-0.14620348811149597, -0.08507151901721954],\n",
       "   'sorcerer': [-0.31668439507484436, 0.08682235330343246],\n",
       "   'believe': [-0.43068820238113403, -0.10825461894273758]}},\n",
       " 'nt': {'vec': [0.2861081063747406, -0.48295286297798157],\n",
       "  'sim': {'joseph': [0.11754206568002701, -0.5034158229827881],\n",
       "   'secretly': [0.12591545283794403, -0.4297703802585602],\n",
       "   'daughter': [0.09747837483882904, -0.34379300475120544],\n",
       "   'young': [0.2273416817188263, -0.15055856108665466],\n",
       "   'appeared': [0.26974475383758545, -0.4258229434490204]}},\n",
       " 'q': {'vec': [-0.35638347268104553, -0.3528476655483246],\n",
       "  'sim': {'withered': [-0.44579216837882996, -0.26631423830986023],\n",
       "   'seven': [-0.5516380667686462, -0.0783514603972435],\n",
       "   'diligently': [-0.22533950209617615, -0.32870155572891235],\n",
       "   'figure': [-0.29273200035095215, -0.1565006524324417],\n",
       "   'clay': [-0.44469204545021057, -0.07567775249481201]}}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_vecs['dream']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04155232 -0.08129367]\n",
      "[ 0.2077616  -0.40646835]\n",
      "\n",
      "\n",
      "[0.02935514 0.34414011]\n",
      "[0.14677569 1.72070055]\n",
      "\n",
      "\n",
      "[0.12654585 0.01668817]\n",
      "[0.63272923 0.08344087]\n",
      "\n",
      "\n",
      "[-0.19904684  0.11736503]\n",
      "[-0.99523418  0.58682515]\n",
      "\n",
      "\n",
      "[-0.07173096  0.30160221]\n",
      "[-0.35865478  1.50801106]\n",
      "\n",
      "\n",
      "[-0.11522597  0.07545491]\n",
      "[-0.57612985  0.37727457]\n",
      "\n",
      "\n",
      "[-0.07025802  0.11172661]\n",
      "[-0.35129011  0.55863304]\n",
      "\n",
      "\n",
      "[-0.10364413  0.21499231]\n",
      "[-0.51822066  1.07496154]\n",
      "\n",
      "\n",
      "[-0.20689237  0.05624179]\n",
      "[-1.03446186  0.28120894]\n",
      "\n",
      "\n",
      "[0.14560908 0.1473939 ]\n",
      "[0.7280454 0.7369695]\n",
      "\n",
      "\n",
      "[0.15197644 0.01621872]\n",
      "[0.75988218 0.08109361]\n",
      "\n",
      "\n",
      "[ 0.02575868 -0.19168114]\n",
      "[ 0.12879342 -0.95840571]\n",
      "\n",
      "\n",
      "[ 0.00542241 -0.52220769]\n",
      "[ 0.02711207 -2.61103846]\n",
      "\n",
      "\n",
      "[-0.14672446 -0.2110628 ]\n",
      "[-0.73362231 -1.05531402]\n",
      "\n",
      "\n",
      "[-0.20358497 -0.09813835]\n",
      "[-1.01792485 -0.49069177]\n",
      "\n",
      "\n",
      "[0.01288436 0.14432516]\n",
      "[0.06442182 0.72162581]\n",
      "\n",
      "\n",
      "[-0.22280198  0.10735411]\n",
      "[-1.11400988  0.53677056]\n",
      "\n",
      "\n",
      "[-0.1662228   0.31254993]\n",
      "[-0.83111402  1.56274967]\n",
      "\n",
      "\n",
      "[-0.05894364  0.28806271]\n",
      "[-0.29471818  1.44031357]\n",
      "\n",
      "\n",
      "[-0.07998472  0.03411452]\n",
      "[-0.39992359  0.17057262]\n",
      "\n",
      "\n",
      "[0.17093116 0.17247502]\n",
      "[0.8546558  0.86237509]\n",
      "\n",
      "\n",
      "[0.4329816  0.07360616]\n",
      "[2.16490798 0.36803078]\n",
      "\n",
      "\n",
      "[0.31351341 0.10908765]\n",
      "[1.56756707 0.54543825]\n",
      "\n",
      "\n",
      "[-0.0708892   0.08393239]\n",
      "[-0.35444602  0.41966194]\n",
      "\n",
      "\n",
      "[ 0.05478324 -0.09827001]\n",
      "[ 0.27391618 -0.49135007]\n",
      "\n",
      "\n",
      "[0.22869302 0.30398071]\n",
      "[1.14346512 1.51990357]\n",
      "\n",
      "\n",
      "[-0.21398179  0.05387238]\n",
      "[-1.06990895  0.26936189]\n",
      "\n",
      "\n",
      "[ 0.25725527 -0.25596234]\n",
      "[ 1.28627636 -1.27981168]\n",
      "\n",
      "\n",
      "[ 0.24528711 -0.10815676]\n",
      "[ 1.22643553 -0.54078379]\n",
      "\n",
      "\n",
      "[0.08769827 0.1652941 ]\n",
      "[0.43849133 0.82647051]\n",
      "\n",
      "\n",
      "[-0.11890502  0.11377124]\n",
      "[-0.59452508  0.56885622]\n",
      "\n",
      "\n",
      "[0.21758025 0.2927462 ]\n",
      "[1.08790126 1.46373101]\n",
      "\n",
      "\n",
      "[-0.05014651  0.11925454]\n",
      "[-0.25073253  0.59627272]\n",
      "\n",
      "\n",
      "[-0.16794546 -0.02544288]\n",
      "[-0.8397273  -0.12721442]\n",
      "\n",
      "\n",
      "[-0.1501794   0.00590087]\n",
      "[-0.75089701  0.02950437]\n",
      "\n",
      "\n",
      "[-0.20507959  0.04826555]\n",
      "[-1.02539796  0.24132773]\n",
      "\n",
      "\n",
      "[0.14757546 0.10601278]\n",
      "[0.73787728 0.53006392]\n",
      "\n",
      "\n",
      "[-0.35473023  0.30029356]\n",
      "[-1.77365113  1.50146779]\n",
      "\n",
      "\n",
      "[-0.11470779  0.15310309]\n",
      "[-0.57353895  0.76551543]\n",
      "\n",
      "\n",
      "[0.26493406 0.13081198]\n",
      "[1.32467028 0.65405991]\n",
      "\n",
      "\n",
      "[ 0.00189447 -0.17368443]\n",
      "[ 0.00947237 -0.86842217]\n",
      "\n",
      "\n",
      "[-0.19326526 -0.14982916]\n",
      "[-0.9663263  -0.74914582]\n",
      "\n",
      "\n",
      "[-0.01473388 -0.02707164]\n",
      "[-0.0736694 -0.1353582]\n",
      "\n",
      "\n",
      "[-0.14625022 -0.03658238]\n",
      "[-0.73125109 -0.1829119 ]\n",
      "\n",
      "\n",
      "[0.17403857 0.05645058]\n",
      "[0.87019287 0.28225291]\n",
      "\n",
      "\n",
      "[0.17367568 0.04896653]\n",
      "[0.86837841 0.24483263]\n",
      "\n",
      "\n",
      "[-0.24621083 -0.04982522]\n",
      "[-1.23105414 -0.24912611]\n",
      "\n",
      "\n",
      "[0.02086934 0.10195179]\n",
      "[0.10434672 0.50975896]\n",
      "\n",
      "\n",
      "[-0.08468141  0.18742695]\n",
      "[-0.42340703  0.93713474]\n",
      "\n",
      "\n",
      "[-0.14707415  0.29272084]\n",
      "[-0.73537074  1.4636042 ]\n",
      "\n",
      "\n",
      "[0.02388966 0.35127386]\n",
      "[0.11944831 1.75636932]\n",
      "\n",
      "\n",
      "[-0.0412845  -0.01561478]\n",
      "[-0.20642251 -0.07807389]\n",
      "\n",
      "\n",
      "[-0.03665709  0.18614373]\n",
      "[-0.18328544  0.93071863]\n",
      "\n",
      "\n",
      "[-0.25007615  0.37273184]\n",
      "[-1.25038076  1.86365921]\n",
      "\n",
      "\n",
      "[ 0.01799856 -0.09255964]\n",
      "[ 0.08999279 -0.46279818]\n",
      "\n",
      "\n",
      "[-0.3132824   0.15080632]\n",
      "[-1.566412   0.7540316]\n",
      "\n",
      "\n",
      "[-0.18251675  0.34287356]\n",
      "[-0.91258377  1.71436782]\n",
      "\n",
      "\n",
      "[-0.1327039  -0.00902651]\n",
      "[-0.6635195  -0.04513257]\n",
      "\n",
      "\n",
      "[-0.17744961 -0.07892007]\n",
      "[-0.88724807 -0.39460034]\n",
      "\n",
      "\n",
      "[-0.33380988 -0.09326035]\n",
      "[-1.66904941 -0.46630175]\n",
      "\n",
      "\n",
      "[ 0.10555075 -0.08547515]\n",
      "[ 0.52775376 -0.42737577]\n",
      "\n",
      "\n",
      "[0.19096858 0.14443498]\n",
      "[0.9548429  0.72217491]\n",
      "\n",
      "\n",
      "[0.30179849 0.21856681]\n",
      "[1.50899246 1.09283406]\n",
      "\n",
      "\n",
      "[ 0.11436072 -0.1714026 ]\n",
      "[ 0.5718036  -0.85701298]\n",
      "\n",
      "\n",
      "[0.10593292 0.00165876]\n",
      "[0.52966461 0.0082938 ]\n",
      "\n",
      "\n",
      "[0.0724934  0.02024501]\n",
      "[0.36246702 0.10122506]\n",
      "\n",
      "\n",
      "[ 0.42008337 -0.01292066]\n",
      "[ 2.10041683 -0.06460328]\n",
      "\n",
      "\n",
      "[-0.02245787  0.2945508 ]\n",
      "[-0.11228934  1.47275399]\n",
      "\n",
      "\n",
      "[-0.12562764  0.1571896 ]\n",
      "[-0.62813818  0.785948  ]\n",
      "\n",
      "\n",
      "[0.1907097  0.21298022]\n",
      "[0.95354848 1.06490109]\n",
      "\n",
      "\n",
      "[0.074939 0.050827]\n",
      "[0.37469499 0.25413498]\n",
      "\n",
      "\n",
      "[0.20450629 0.27145207]\n",
      "[1.02253146 1.35726037]\n",
      "\n",
      "\n",
      "[-0.0745346  -0.10735866]\n",
      "[-0.37267301 -0.53679332]\n",
      "\n",
      "\n",
      "[-0.0661152  -0.08061957]\n",
      "[-0.33057599 -0.40309787]\n",
      "\n",
      "\n",
      "[-0.01744293 -0.05935568]\n",
      "[-0.08721467 -0.29677838]\n",
      "\n",
      "\n",
      "[0.00823341 0.17914742]\n",
      "[0.04116707 0.89573709]\n",
      "\n",
      "\n",
      "[ 0.10054337 -0.18448849]\n",
      "[ 0.50271686 -0.92244247]\n",
      "\n",
      "\n",
      "[0.08148719 0.03958354]\n",
      "[0.40743593 0.19791769]\n",
      "\n",
      "\n",
      "[-0.09566799  0.18512115]\n",
      "[-0.47833994  0.92560573]\n",
      "\n",
      "\n",
      "[ 0.05532373 -0.19446186]\n",
      "[ 0.27661867 -0.97230928]\n",
      "\n",
      "\n",
      "[-0.00830829  0.03777864]\n",
      "[-0.04154146  0.18889318]\n",
      "\n",
      "\n",
      "[0.02092832 0.20035323]\n",
      "[0.10464162 1.00176614]\n",
      "\n",
      "\n",
      "[0.20269558 0.23705014]\n",
      "[1.01347789 1.18525069]\n",
      "\n",
      "\n",
      "[0.07831025 0.09053839]\n",
      "[0.39155126 0.45269195]\n",
      "\n",
      "\n",
      "[0.11791283 0.22857847]\n",
      "[0.58956414 1.14289235]\n",
      "\n",
      "\n",
      "[ 0.15012813 -0.17969616]\n",
      "[ 0.75064063 -0.89848079]\n",
      "\n",
      "\n",
      "[-0.16057155 -0.1641704 ]\n",
      "[-0.80285773 -0.820852  ]\n",
      "\n",
      "\n",
      "[0.00095302 0.1482475 ]\n",
      "[0.00476509 0.74123748]\n",
      "\n",
      "\n",
      "[-0.21032983  0.06362228]\n",
      "[-1.05164915  0.3181114 ]\n",
      "\n",
      "\n",
      "[-0.36524095  0.17861621]\n",
      "[-1.82620473  0.89308105]\n",
      "\n",
      "\n",
      "[-0.16734114 -0.05832733]\n",
      "[-0.83670571 -0.29163664]\n",
      "\n",
      "\n",
      "[0.17698315 0.1491222 ]\n",
      "[0.88491574 0.74561099]\n",
      "\n",
      "\n",
      "[-0.10906783  0.19803529]\n",
      "[-0.54533914  0.99017644]\n",
      "\n",
      "\n",
      "[0.29012409 0.06407312]\n",
      "[1.45062044 0.3203656 ]\n",
      "\n",
      "\n",
      "[0.19157395 0.39474774]\n",
      "[0.95786974 1.97373869]\n",
      "\n",
      "\n",
      "[0.01165271 0.13072371]\n",
      "[0.05826354 0.65361857]\n",
      "\n",
      "\n",
      "[-0.03157565  0.20032665]\n",
      "[-0.15787825  1.00163326]\n",
      "\n",
      "\n",
      "[0.21071818 0.19704331]\n",
      "[1.05359092 0.98521657]\n",
      "\n",
      "\n",
      "[-0.03463268  0.30378722]\n",
      "[-0.17316341  1.51893608]\n",
      "\n",
      "\n",
      "[-0.00262535  0.04411075]\n",
      "[-0.01312673  0.22055373]\n",
      "\n",
      "\n",
      "[ 0.10649736 -0.20114262]\n",
      "[ 0.53248681 -1.00571312]\n",
      "\n",
      "\n",
      "[-0.18935552 -0.23623239]\n",
      "[-0.94677761 -1.18116193]\n",
      "\n",
      "\n",
      "[-0.49386878 -0.0442071 ]\n",
      "[-2.46934392 -0.22103548]\n",
      "\n",
      "\n",
      "[0.13489588 0.02756479]\n",
      "[0.67447938 0.13782397]\n",
      "\n",
      "\n",
      "[-0.23764246 -0.0102521 ]\n",
      "[-1.18821228 -0.05126052]\n",
      "\n",
      "\n",
      "[0.1389072  0.27229912]\n",
      "[0.69453602 1.36149561]\n",
      "\n",
      "\n",
      "[0.24128231 0.05051466]\n",
      "[1.20641153 0.25257328]\n",
      "\n",
      "\n",
      "[0.22139051 0.14839489]\n",
      "[1.10695256 0.74197447]\n",
      "\n",
      "\n",
      "[-0.18550362 -0.0917668 ]\n",
      "[-0.92751808 -0.458834  ]\n",
      "\n",
      "\n",
      "[0.17184911 0.0521793 ]\n",
      "[0.85924557 0.26089651]\n",
      "\n",
      "\n",
      "[0.227471   0.14365229]\n",
      "[1.13735501 0.71826145]\n",
      "\n",
      "\n",
      "[0.0358445  0.14825539]\n",
      "[0.17922252 0.74127696]\n",
      "\n",
      "\n",
      "[0.03919575 0.07393928]\n",
      "[0.19597873 0.36969639]\n",
      "\n",
      "\n",
      "[-0.14943498 -0.16404647]\n",
      "[-0.74717492 -0.82023237]\n",
      "\n",
      "\n",
      "[-0.0239656   0.11771862]\n",
      "[-0.11982799  0.58859311]\n",
      "\n",
      "\n",
      "[-0.03910687 -0.08871251]\n",
      "[-0.19553435 -0.44356257]\n",
      "\n",
      "\n",
      "[-0.11358289  0.02651788]\n",
      "[-0.56791445  0.13258941]\n",
      "\n",
      "\n",
      "[-0.15516933 -0.11531277]\n",
      "[-0.77584663 -0.57656385]\n",
      "\n",
      "\n",
      "[0.19795175 0.04005386]\n",
      "[0.98975876 0.2002693 ]\n",
      "\n",
      "\n",
      "[-0.0872793 -0.2632856]\n",
      "[-0.43639652 -1.316428  ]\n",
      "\n",
      "\n",
      "[-0.10577919 -0.05233107]\n",
      "[-0.52889593 -0.26165535]\n",
      "\n",
      "\n",
      "[0.11874843 0.12850604]\n",
      "[0.59374217 0.64253018]\n",
      "\n",
      "\n",
      "[ 0.05109476 -0.09137189]\n",
      "[ 0.25547378 -0.45685946]\n",
      "\n",
      "\n",
      "[-0.11938615  0.08052198]\n",
      "[-0.59693076  0.4026099 ]\n",
      "\n",
      "\n",
      "[-0.23338996 -0.11455499]\n",
      "[-1.16694979 -0.57277496]\n",
      "\n",
      "\n",
      "[-0.16856604 -0.02046296]\n",
      "[-0.8428302 -0.1023148]\n",
      "\n",
      "\n",
      "[-0.16019265  0.05318248]\n",
      "[-0.80096327  0.26591241]\n",
      "\n",
      "\n",
      "[-0.18862973  0.13915986]\n",
      "[-0.94314866  0.69579929]\n",
      "\n",
      "\n",
      "[-0.05876642  0.3323943 ]\n",
      "[-0.29383212  1.66197151]\n",
      "\n",
      "\n",
      "[-0.01636335  0.05712992]\n",
      "[-0.08181676  0.2856496 ]\n",
      "\n",
      "\n",
      "[-0.0894087   0.08653343]\n",
      "[-0.44704348  0.43266714]\n",
      "\n",
      "\n",
      "[-0.19525459  0.27449621]\n",
      "[-0.97627297  1.37248103]\n",
      "\n",
      "\n",
      "[0.13104397 0.02414611]\n",
      "[0.65521985 0.12073055]\n",
      "\n",
      "\n",
      "[0.06365147 0.19634701]\n",
      "[0.31825736 0.98173507]\n",
      "\n",
      "\n",
      "[-0.08830857  0.27716991]\n",
      "[-0.44154286  1.38584957]\n",
      "\n",
      "\n",
      "[ 0.04895046 -0.0314137 ]\n",
      "[ 0.24475232 -0.15706852]\n",
      "\n",
      "\n",
      "[0.06912783 0.13585514]\n",
      "[0.34563914 0.67927569]\n",
      "\n",
      "\n",
      "[0.11399013 0.08251226]\n",
      "[0.56995064 0.4125613 ]\n",
      "\n",
      "\n",
      "[0.28690162 0.10008645]\n",
      "[1.43450812 0.50043225]\n",
      "\n",
      "\n",
      "[-2.11000443e-05  3.04632828e-01]\n",
      "[-1.05500221e-04  1.52316414e+00]\n",
      "\n",
      "\n",
      "[0.01062849 0.16366349]\n",
      "[0.05314246 0.81831746]\n",
      "\n",
      "\n",
      "[-0.03701377  0.00469831]\n",
      "[-0.18506885  0.02349153]\n",
      "\n",
      "\n",
      "[-0.14201236  0.0042299 ]\n",
      "[-0.71006179  0.02114952]\n",
      "\n",
      "\n",
      "[0.0839389  0.05466403]\n",
      "[0.41969448 0.27332015]\n",
      "\n",
      "\n",
      "[-0.17266837 -0.03088748]\n",
      "[-0.86334184 -0.15443742]\n",
      "\n",
      "\n",
      "[0.11163418 0.09364023]\n",
      "[0.5581709  0.46820115]\n",
      "\n",
      "\n",
      "[0.1938711  0.34671784]\n",
      "[0.96935548 1.73358921]\n",
      "\n",
      "\n",
      "[-0.22867273  0.11656304]\n",
      "[-1.14336364  0.58281522]\n",
      "\n",
      "\n",
      "[ 0.10322954 -0.2395673 ]\n",
      "[ 0.51614769 -1.19783651]\n",
      "\n",
      "\n",
      "[ 0.03565918 -0.24202805]\n",
      "[ 0.1782959  -1.21014025]\n",
      "\n",
      "\n",
      "[0.0752132  0.12877909]\n",
      "[0.37606601 0.64389547]\n",
      "\n",
      "\n",
      "[ 0.17955894 -0.02724388]\n",
      "[ 0.8977947  -0.13621941]\n",
      "\n",
      "\n",
      "[-0.02103783  0.13269068]\n",
      "[-0.10518916  0.66345342]\n",
      "\n",
      "\n",
      "[-0.27242739 -0.1372022 ]\n",
      "[-1.36213694 -0.68601102]\n",
      "\n",
      "\n",
      "[-0.14508218  0.18767101]\n",
      "[-0.7254109   0.93835507]\n",
      "\n",
      "\n",
      "[0.09022878 0.29578833]\n",
      "[0.45114391 1.47894166]\n",
      "\n",
      "\n",
      "[0.24991502 0.16675607]\n",
      "[1.24957509 0.83378034]\n",
      "\n",
      "\n",
      "[-0.10346682  0.30594435]\n",
      "[-0.51733408  1.52972177]\n",
      "\n",
      "\n",
      "[0.4025179  0.11828083]\n",
      "[2.0125895  0.59140416]\n",
      "\n",
      "\n",
      "[0.14967166 0.0411729 ]\n",
      "[0.74835829 0.2058645 ]\n",
      "\n",
      "\n",
      "[-0.09211147  0.11672071]\n",
      "[-0.46055736  0.58360353]\n",
      "\n",
      "\n",
      "[0.00558412 0.15349919]\n",
      "[0.0279206  0.76749593]\n",
      "\n",
      "\n",
      "[-0.25804371 -0.03616585]\n",
      "[-1.29021857 -0.18082924]\n",
      "\n",
      "\n",
      "[-0.23584878 -0.14352353]\n",
      "[-1.17924388 -0.71761763]\n",
      "\n",
      "\n",
      "[-0.02421983 -0.02827129]\n",
      "[-0.12109915 -0.14135644]\n",
      "\n",
      "\n",
      "[ 0.00232704 -0.17192207]\n",
      "[ 0.0116352  -0.85961036]\n",
      "\n",
      "\n",
      "[-0.21132961 -0.04484293]\n",
      "[-1.05664803 -0.22421464]\n",
      "\n",
      "\n",
      "[-0.23756723  0.1921683 ]\n",
      "[-1.18783616  0.96084148]\n",
      "\n",
      "\n",
      "[ 0.04855941 -0.01525229]\n",
      "[ 0.24279706 -0.07626146]\n",
      "\n",
      "\n",
      "[0.03164841 0.06385493]\n",
      "[0.15824206 0.31927466]\n",
      "\n",
      "\n",
      "[-0.04307295  0.0258306 ]\n",
      "[-0.21536477  0.12915298]\n",
      "\n",
      "\n",
      "[0.10986961 0.24294934]\n",
      "[0.54934807 1.21474668]\n",
      "\n",
      "\n",
      "[ 0.14982225 -0.0230172 ]\n",
      "[ 0.74911125 -0.11508599]\n",
      "\n",
      "\n",
      "[0.07468452 0.08539301]\n",
      "[0.37342258 0.42696506]\n",
      "\n",
      "\n",
      "[ 0.3088515 -0.1922746]\n",
      "[ 1.54425748 -0.961373  ]\n",
      "\n",
      "\n",
      "[-0.06604294 -0.04555526]\n",
      "[-0.33021468 -0.22777628]\n",
      "\n",
      "\n",
      "[-0.09046806 -0.10965639]\n",
      "[-0.45234028 -0.54828197]\n",
      "\n",
      "\n",
      "[-0.2355546  -0.11730178]\n",
      "[-1.17777302 -0.58650889]\n",
      "\n",
      "\n",
      "[-0.12714474 -0.03511765]\n",
      "[-0.63572369 -0.17558824]\n",
      "\n",
      "\n",
      "[-0.15951131 -0.10941923]\n",
      "[-0.79755656 -0.54709613]\n",
      "\n",
      "\n",
      "[-0.15261157  0.06989925]\n",
      "[-0.76305784  0.34949623]\n",
      "\n",
      "\n",
      "[-0.15682815 -0.009425  ]\n",
      "[-0.78414075 -0.047125  ]\n",
      "\n",
      "\n",
      "[0.15456655 0.10287966]\n",
      "[0.77283274 0.51439829]\n",
      "\n",
      "\n",
      "[0.06969896 0.20889591]\n",
      "[0.3484948  1.04447953]\n",
      "\n",
      "\n",
      "[-0.12364711  0.06101985]\n",
      "[-0.61823554  0.30509926]\n",
      "\n",
      "\n",
      "[-0.17294648  0.00304861]\n",
      "[-0.86473241  0.01524307]\n",
      "\n",
      "\n",
      "[0.05159086 0.14812243]\n",
      "[0.2579543  0.74061213]\n",
      "\n",
      "\n",
      "[-0.18526092 -0.04991361]\n",
      "[-0.92630461 -0.24956804]\n",
      "\n",
      "\n",
      "[-0.2661334   0.14038885]\n",
      "[-1.33066699  0.70194427]\n",
      "\n",
      "\n",
      "[-0.16917732 -0.00797789]\n",
      "[-0.84588662 -0.03988944]\n",
      "\n",
      "\n",
      "[-0.29654739  0.10115379]\n",
      "[-1.48273693  0.50576895]\n",
      "\n",
      "\n",
      "[-0.14184242  0.07424176]\n",
      "[-0.70921212  0.37120879]\n",
      "\n",
      "\n",
      "[-0.27768533 -0.09218186]\n",
      "[-1.38842664 -0.46090931]\n",
      "\n",
      "\n",
      "[ 0.08589229 -0.41601186]\n",
      "[ 0.42946145 -2.08005928]\n",
      "\n",
      "\n",
      "[-0.27297435 -0.20911902]\n",
      "[-1.36487174 -1.04559511]\n",
      "\n",
      "\n",
      "[ 0.0519281  -0.12181088]\n",
      "[ 0.25964051 -0.60905442]\n",
      "\n",
      "\n",
      "[-0.01834172  0.02124444]\n",
      "[-0.0917086   0.10622218]\n",
      "\n",
      "\n",
      "[-0.08121306 -0.02392051]\n",
      "[-0.40606529 -0.11960253]\n",
      "\n",
      "\n",
      "[ 0.24749821 -0.08335593]\n",
      "[ 1.23749103 -0.41677967]\n",
      "\n",
      "\n",
      "[ 0.02996719 -0.04372914]\n",
      "[ 0.14983594 -0.21864571]\n",
      "\n",
      "\n",
      "[-0.07487273 -0.09381798]\n",
      "[-0.37436366 -0.46908991]\n",
      "\n",
      "\n",
      "[-0.05806869 -0.02667932]\n",
      "[-0.29034346 -0.13339661]\n",
      "\n",
      "\n",
      "[-0.0181762   0.05460048]\n",
      "[-0.09088099  0.27300239]\n",
      "\n",
      "\n",
      "[0.33489707 0.06941578]\n",
      "[1.67448536 0.34707889]\n",
      "\n",
      "\n",
      "[ 0.08713558 -0.11923007]\n",
      "[ 0.43567792 -0.59615036]\n",
      "\n",
      "\n",
      "[-0.22986194  0.06010246]\n",
      "[-1.14930972  0.30051229]\n",
      "\n",
      "\n",
      "[-0.29578    -0.10804836]\n",
      "[-1.47890002 -0.54024182]\n",
      "\n",
      "\n",
      "[-0.24714082 -0.11860308]\n",
      "[-1.23570412 -0.5930154 ]\n",
      "\n",
      "\n",
      "[-0.11716405 -0.07535827]\n",
      "[-0.58582023 -0.37679136]\n",
      "\n",
      "\n",
      "[-0.26019809  0.10005423]\n",
      "[-1.30099043  0.50027117]\n",
      "\n",
      "\n",
      "[0.28160594 0.21111578]\n",
      "[1.40802972 1.05557892]\n",
      "\n",
      "\n",
      "[ 0.18550404 -0.00871161]\n",
      "[ 0.92752019 -0.04355807]\n",
      "\n",
      "\n",
      "[-0.0504009   0.10612033]\n",
      "[-0.25200449  0.53060163]\n",
      "\n",
      "\n",
      "[-0.20938437  0.06806436]\n",
      "[-1.04692183  0.34032181]\n",
      "\n",
      "\n",
      "[0.08630648 0.04897568]\n",
      "[0.43153238 0.24487842]\n",
      "\n",
      "\n",
      "[0.06615537 0.32827182]\n",
      "[0.33077687 1.64135911]\n",
      "\n",
      "\n",
      "[ 0.04494694 -0.03679429]\n",
      "[ 0.22473469 -0.18397145]\n",
      "\n",
      "\n",
      "[-0.0426642   0.23296236]\n",
      "[-0.213321    1.16481178]\n",
      "\n",
      "\n",
      "[0.13504028 0.06002466]\n",
      "[0.67520142 0.30012328]\n",
      "\n",
      "\n",
      "[ 0.04471049 -0.1081591 ]\n",
      "[ 0.22355244 -0.54079548]\n",
      "\n",
      "\n",
      "[-0.02179079  0.0957121 ]\n",
      "[-0.10895394  0.47856048]\n",
      "\n",
      "\n",
      "[0.12680688 0.16793922]\n",
      "[0.63403438 0.83969608]\n",
      "\n",
      "\n",
      "[-0.04349276 -0.19505471]\n",
      "[-0.21746382 -0.97527355]\n",
      "\n",
      "\n",
      "[ 0.01923996 -0.11781037]\n",
      "[ 0.09619981 -0.58905184]\n",
      "\n",
      "\n",
      "[ 0.36292244 -0.12682542]\n",
      "[ 1.81461222 -0.63412711]\n",
      "\n",
      "\n",
      "[-0.05109476  0.09137189]\n",
      "[-0.25547378  0.45685946]\n",
      "\n",
      "\n",
      "[-0.10593292 -0.00165876]\n",
      "[-0.52966461 -0.0082938 ]\n",
      "\n",
      "\n",
      "[-0.12908918 -0.10431719]\n",
      "[-0.64544588 -0.52158594]\n",
      "\n",
      "\n",
      "[-0.15687394  0.03904082]\n",
      "[-0.78436971  0.19520411]\n",
      "\n",
      "\n",
      "[0.14194443 0.10819009]\n",
      "[0.70972213 0.54095043]\n",
      "\n",
      "\n",
      "[0.07411782 0.19740114]\n",
      "[0.37058912 0.98700568]\n",
      "\n",
      "\n",
      "[0.30109422 0.10329735]\n",
      "[1.5054711  0.51648676]\n",
      "\n",
      "\n",
      "[-0.00172797  0.46312977]\n",
      "[-0.00863984  2.31564885]\n",
      "\n",
      "\n",
      "[-0.32242505 -0.04782087]\n",
      "[-1.61212526 -0.23910433]\n",
      "\n",
      "\n",
      "[0.34421514 0.24227825]\n",
      "[1.7210757  1.21139124]\n",
      "\n",
      "\n",
      "[0.07338381 0.06966206]\n",
      "[0.36691904 0.34831028]\n",
      "\n",
      "\n",
      "[ 0.05033423 -0.11334123]\n",
      "[ 0.25167115 -0.56670614]\n",
      "\n",
      "\n",
      "[ 0.28930731 -0.12789987]\n",
      "[ 1.44653656 -0.63949935]\n",
      "\n",
      "\n",
      "[-0.26221202 -0.07993491]\n",
      "[-1.31106012 -0.39967455]\n",
      "\n",
      "\n",
      "[-0.07694152 -0.08222207]\n",
      "[-0.38470758 -0.41111037]\n",
      "\n",
      "\n",
      "[0.11427817 0.05700781]\n",
      "[0.57139084 0.28503906]\n",
      "\n",
      "\n",
      "[-0.05964598 -0.03659909]\n",
      "[-0.2982299  -0.18299546]\n",
      "\n",
      "\n",
      "[ 0.15539554 -0.08113409]\n",
      "[ 0.77697769 -0.40567044]\n",
      "\n",
      "\n",
      "[0.02350041 0.19062907]\n",
      "[0.11750206 0.95314536]\n",
      "\n",
      "\n",
      "[0.24324115 0.02738917]\n",
      "[1.21620573 0.13694584]\n",
      "\n",
      "\n",
      "[0.17881346 0.02300301]\n",
      "[0.89406732 0.11501506]\n",
      "\n",
      "\n",
      "[0.41171204 0.20133419]\n",
      "[2.05856018 1.00667097]\n",
      "\n",
      "\n",
      "[0.02508563 0.25600025]\n",
      "[0.12542814 1.28000125]\n",
      "\n",
      "\n",
      "[ 0.09055197 -0.2254515 ]\n",
      "[ 0.45275986 -1.1272575 ]\n",
      "\n",
      "\n",
      "[-0.03526026  0.45477732]\n",
      "[-0.1763013   2.27388658]\n",
      "\n",
      "\n",
      "[-0.24990773  0.05857795]\n",
      "[-1.24953866  0.29288977]\n",
      "\n",
      "\n",
      "[-0.15197644 -0.01621872]\n",
      "[-0.75988218 -0.08109361]\n",
      "\n",
      "\n",
      "[-0.1007596  -0.25573707]\n",
      "[-0.50379798 -1.27868537]\n",
      "\n",
      "\n",
      "[-0.2987009  -0.22728153]\n",
      "[-1.49350449 -1.13640763]\n",
      "\n",
      "\n",
      "[ 0.03684217 -0.25938968]\n",
      "[ 0.18421084 -1.29694838]\n",
      "\n",
      "\n",
      "[ 0.1530986  -0.02239785]\n",
      "[ 0.76549301 -0.11198925]\n",
      "\n",
      "\n",
      "[0.31913627 0.20996411]\n",
      "[1.59568133 1.04982054]\n",
      "\n",
      "\n",
      "[0.21766115 0.07564172]\n",
      "[1.08830573 0.37820859]\n",
      "\n",
      "\n",
      "[0.02026323 0.15722012]\n",
      "[0.10131616 0.78610061]\n",
      "\n",
      "\n",
      "[ 0.11245696 -0.07789098]\n",
      "[ 0.56228479 -0.38945492]\n",
      "\n",
      "\n",
      "[0.07128238 0.17730184]\n",
      "[0.35641189 0.88650919]\n",
      "\n",
      "\n",
      "[0.08508484 0.19460861]\n",
      "[0.42542419 0.97304307]\n",
      "\n",
      "\n",
      "[-0.05077107  0.05617274]\n",
      "[-0.25385536  0.28086372]\n",
      "\n",
      "\n",
      "[0.21641346 0.14518926]\n",
      "[1.08206729 0.72594628]\n",
      "\n",
      "\n",
      "[0.43574189 0.27252392]\n",
      "[2.17870946 1.36261962]\n",
      "\n",
      "\n",
      "[-0.27337074 -0.13671067]\n",
      "[-1.36685368 -0.68355333]\n",
      "\n",
      "\n",
      "[-0.29505765 -0.19435921]\n",
      "[-1.47528823 -0.97179607]\n",
      "\n",
      "\n",
      "[-0.60539769 -0.02955267]\n",
      "[-3.02698847 -0.14776336]\n",
      "\n",
      "\n",
      "[-0.27718811  0.19406207]\n",
      "[-1.38594057  0.97031035]\n",
      "\n",
      "\n",
      "[-0.553025   -0.00423095]\n",
      "[-2.765125   -0.02115477]\n",
      "\n",
      "\n",
      "[ 0.03510165 -0.23696589]\n",
      "[ 0.17550826 -1.18482944]\n",
      "\n",
      "\n",
      "[-0.03801051 -0.05248545]\n",
      "[-0.19005254 -0.26242726]\n",
      "\n",
      "\n",
      "[-0.21159679 -0.16620759]\n",
      "[-1.05798393 -0.83103793]\n",
      "\n",
      "\n",
      "[ 0.24804047 -0.10139854]\n",
      "[ 1.24020234 -0.50699271]\n",
      "\n",
      "\n",
      "[-0.07386246 -0.14437937]\n",
      "[-0.36931232 -0.72189683]\n",
      "\n",
      "\n",
      "[ 0.03936148 -0.03120916]\n",
      "[ 0.19680738 -0.15604578]\n",
      "\n",
      "\n",
      "[0.01233199 0.15987642]\n",
      "[0.06165996 0.79938211]\n",
      "\n",
      "\n",
      "[-0.10825545 -0.22977819]\n",
      "[-0.54127723 -1.14889096]\n",
      "\n",
      "\n",
      "[0.07925397 0.05007064]\n",
      "[0.39626986 0.25035322]\n",
      "\n",
      "\n",
      "[-0.13090152 -0.06285354]\n",
      "[-0.65450758 -0.31426769]\n",
      "\n",
      "\n",
      "[ 0.02606428 -0.05765363]\n",
      "[ 0.13032138 -0.28826814]\n",
      "\n",
      "\n",
      "[-0.04304233  0.0261231 ]\n",
      "[-0.21521166  0.1306155 ]\n",
      "\n",
      "\n",
      "[-0.01812313  0.11844713]\n",
      "[-0.09061567  0.59223563]\n",
      "\n",
      "\n",
      "[-0.04245904 -0.06637139]\n",
      "[-0.2122952  -0.33185694]\n",
      "\n",
      "\n",
      "[-0.42174226 -0.34509464]\n",
      "[-2.1087113  -1.72547322]\n",
      "\n",
      "\n",
      "[-0.1530986   0.02239785]\n",
      "[-0.76549301  0.11198925]\n",
      "\n",
      "\n",
      "[0.15304039 0.07577081]\n",
      "[0.76520197 0.37885405]\n",
      "\n",
      "\n",
      "[ 0.14071463 -0.08303306]\n",
      "[ 0.70357315 -0.41516528]\n",
      "\n",
      "\n",
      "[0.1797695  0.03579297]\n",
      "[0.89884751 0.17896483]\n",
      "\n",
      "\n",
      "[0.16603766 0.23236196]\n",
      "[0.83018832 1.16180979]\n",
      "\n",
      "\n",
      "[-0.07128238 -0.17730184]\n",
      "[-0.35641189 -0.88650919]\n",
      "\n",
      "\n",
      "[0.01380246 0.01730677]\n",
      "[0.06901231 0.08653387]\n",
      "\n",
      "\n",
      "[0.30756363 0.09462118]\n",
      "[1.53781816 0.47310591]\n",
      "\n",
      "\n",
      "[-0.12107625  0.11133671]\n",
      "[-0.60538125  0.55668354]\n",
      "\n",
      "\n",
      "[0.36445951 0.09522209]\n",
      "[1.82229757 0.47611043]\n",
      "\n",
      "\n",
      "[-0.02168691 -0.05764855]\n",
      "[-0.10843456 -0.28824273]\n",
      "\n",
      "\n",
      "[0.27337074 0.13671067]\n",
      "[1.36685368 0.68355333]\n",
      "\n",
      "\n",
      "[-0.33202696  0.10715799]\n",
      "[-1.66013479  0.53578997]\n",
      "\n",
      "\n",
      "[-0.00381738  0.33077274]\n",
      "[-0.0190869   1.65386368]\n",
      "\n",
      "\n",
      "[-0.2385374  -0.03356272]\n",
      "[-1.192687  -0.1678136]\n",
      "\n",
      "\n",
      "[ 0.10392258 -0.14271261]\n",
      "[ 0.51961288 -0.71356304]\n",
      "\n",
      "\n",
      "[0.07105725 0.08130369]\n",
      "[0.35528623 0.40651843]\n",
      "\n",
      "\n",
      "[0.12420302 0.00991362]\n",
      "[0.62101508 0.04956808]\n",
      "\n",
      "\n",
      "[0.07096307 0.34701067]\n",
      "[0.35481535 1.73505336]\n",
      "\n",
      "\n",
      "[-0.09344599  0.2295002 ]\n",
      "[-0.46722993  1.14750102]\n",
      "\n",
      "\n",
      "[ 0.24346567 -0.13090113]\n",
      "[ 1.21732837 -0.65450564]\n",
      "\n",
      "\n",
      "[-0.13004062  0.1579693 ]\n",
      "[-0.6502031   0.78984648]\n",
      "\n",
      "\n",
      "[-0.15668202 -0.28372106]\n",
      "[-0.78341009 -1.4186053 ]\n",
      "\n",
      "\n",
      "[-0.17222479 -0.15984365]\n",
      "[-0.86112397 -0.79921827]\n",
      "\n",
      "\n",
      "[-0.28825832  0.10921621]\n",
      "[-1.44129159  0.54608107]\n",
      "\n",
      "\n",
      "[ 0.0107196  -0.23606516]\n",
      "[ 0.05359799 -1.18032582]\n",
      "\n",
      "\n",
      "[-0.35029195 -0.21420933]\n",
      "[-1.75145976 -1.07104663]\n",
      "\n",
      "\n",
      "[ 0.06034128 -0.18909684]\n",
      "[ 0.30170642 -0.94548419]\n",
      "\n",
      "\n",
      "[-0.05134589 -0.03911179]\n",
      "[-0.25672946 -0.19555897]\n",
      "\n",
      "\n",
      "[-0.06226086 -0.29879028]\n",
      "[-0.31130429 -1.49395138]\n",
      "\n",
      "\n",
      "[0.05550832 0.03159663]\n",
      "[0.27754162 0.15798315]\n",
      "\n",
      "\n",
      "[-0.06057142 -0.05325776]\n",
      "[-0.3028571  -0.26628882]\n",
      "\n",
      "\n",
      "[0.30128729 0.35848652]\n",
      "[1.50643646 1.79243259]\n",
      "\n",
      "\n",
      "[0.16033409 0.09461728]\n",
      "[0.80167047 0.47308639]\n",
      "\n",
      "\n",
      "[0.21202379 0.15190443]\n",
      "[1.06011896 0.75952217]\n",
      "\n",
      "\n",
      "[0.55417067 0.01157549]\n",
      "[2.77085334 0.05787745]\n",
      "\n",
      "\n",
      "[0.23762536 0.12848306]\n",
      "[1.1881268  0.64241529]\n",
      "\n",
      "\n",
      "[-0.05216171 -0.06223789]\n",
      "[-0.26080854 -0.31118944]\n",
      "\n",
      "\n",
      "[ 0.37559161 -0.05393116]\n",
      "[ 1.87795803 -0.26965581]\n",
      "\n",
      "\n",
      "[-0.21462256 -0.04142947]\n",
      "[-1.07311279 -0.20714737]\n",
      "\n",
      "\n",
      "[-0.0363543  -0.36001389]\n",
      "[-0.18177148 -1.80006947]\n",
      "\n",
      "\n",
      "[ 0.05257436 -0.1540383 ]\n",
      "[ 0.2628718  -0.77019148]\n",
      "\n",
      "\n",
      "[-0.05348522 -0.21439542]\n",
      "[-0.26742609 -1.07197709]\n",
      "\n",
      "\n",
      "[ 0.30565443 -0.40687927]\n",
      "[ 1.52827215 -2.03439634]\n",
      "\n",
      "\n",
      "[-0.19258397 -0.31977792]\n",
      "[-0.96291984 -1.5988896 ]\n",
      "\n",
      "\n",
      "[0.18854511 0.41007054]\n",
      "[0.94272554 2.05035272]\n",
      "\n",
      "\n",
      "[-0.07640852 -0.16666674]\n",
      "[-0.3820426 -0.8333337]\n",
      "\n",
      "\n",
      "[ 0.06646217 -0.00901392]\n",
      "[ 0.33231087 -0.04506962]\n",
      "\n",
      "\n",
      "[-0.22261135 -0.08923048]\n",
      "[-1.11305676 -0.4461524 ]\n",
      "\n",
      "\n",
      "[-0.2929108   0.02292641]\n",
      "[-1.464554    0.11463203]\n",
      "\n",
      "\n",
      "[-0.21181786 -0.17828325]\n",
      "[-1.0590893  -0.89141626]\n",
      "\n",
      "\n",
      "[-0.05979899  0.18573409]\n",
      "[-0.29899493  0.92867043]\n",
      "\n",
      "\n",
      "[-0.10112634  0.02270067]\n",
      "[-0.50563172  0.11350337]\n",
      "\n",
      "\n",
      "[-0.09236619  0.10056254]\n",
      "[-0.46183094  0.50281268]\n",
      "\n",
      "\n",
      "[-0.08085749  0.15418176]\n",
      "[-0.40428743  0.77090878]\n",
      "\n",
      "\n",
      "[-0.3811263  -0.01131478]\n",
      "[-1.9056315 -0.0565739]\n",
      "\n",
      "\n",
      "[ 0.15323116 -0.28175091]\n",
      "[ 0.76615579 -1.40875456]\n",
      "\n",
      "\n",
      "[-0.19137073 -0.23736815]\n",
      "[-0.95685365 -1.18684077]\n",
      "\n",
      "\n",
      "[0.04103802 0.01951888]\n",
      "[0.20519011 0.09759441]\n",
      "\n",
      "\n",
      "[-0.09270134 -0.44677821]\n",
      "[-0.46350669 -2.23389104]\n",
      "\n",
      "\n",
      "[0.30674358 0.34393433]\n",
      "[1.53371792 1.71967167]\n",
      "\n",
      "\n",
      "[-0.01835218  0.28718469]\n",
      "[-0.0917609   1.43592346]\n",
      "\n",
      "\n",
      "[-0.06098277  0.10239923]\n",
      "[-0.30491386  0.51199616]\n",
      "\n",
      "\n",
      "[-0.04294369  0.0728107 ]\n",
      "[-0.21471847  0.36405349]\n",
      "\n",
      "\n",
      "[0.24769103 0.29362469]\n",
      "[1.23845514 1.46812346]\n",
      "\n",
      "\n",
      "[0.05709144 0.26508271]\n",
      "[0.28545718 1.32541355]\n",
      "\n",
      "\n",
      "[ 0.13928515 -0.12000766]\n",
      "[ 0.69642574 -0.60003828]\n",
      "\n",
      "\n",
      "[-0.06372043  0.13955946]\n",
      "[-0.31860214  0.69779729]\n",
      "\n",
      "\n",
      "[-0.24531495  0.24018397]\n",
      "[-1.22657474  1.20091986]\n",
      "\n",
      "\n",
      "[-0.17105266  0.18794451]\n",
      "[-0.85526329  0.93972255]\n",
      "\n",
      "\n",
      "[-0.06436721  0.13241899]\n",
      "[-0.32183606  0.66209496]\n",
      "\n",
      "\n",
      "[0.14032309 0.30268425]\n",
      "[0.70161544 1.51342125]\n",
      "\n",
      "\n",
      "[-0.341067    0.06888229]\n",
      "[-1.70533501  0.34441144]\n",
      "\n",
      "\n",
      "[-0.23121911 -0.10679586]\n",
      "[-1.15609553 -0.5339793 ]\n",
      "\n",
      "\n",
      "[0.25251622 0.00141446]\n",
      "[1.26258112 0.00707228]\n",
      "\n",
      "\n",
      "[-0.06807068  0.20143101]\n",
      "[-0.3403534   1.00715507]\n",
      "\n",
      "\n",
      "[ 0.16556361 -0.1826364 ]\n",
      "[ 0.82781806 -0.91318201]\n",
      "\n",
      "\n",
      "[-0.17456818 -0.2573024 ]\n",
      "[-0.87284088 -1.28651198]\n",
      "\n",
      "\n",
      "[-0.11174849 -0.34120338]\n",
      "[-0.55874243 -1.70601692]\n",
      "\n",
      "\n",
      "[-0.04047722 -0.09780803]\n",
      "[-0.20238608 -0.48904013]\n",
      "\n",
      "\n",
      "[-0.39557795  0.06409762]\n",
      "[-1.97788976  0.3204881 ]\n",
      "\n",
      "\n",
      "[-0.4258828  -0.03076463]\n",
      "[-2.12941401 -0.15382316]\n",
      "\n",
      "\n",
      "[-0.26734354  0.04042677]\n",
      "[-1.33671768  0.20213387]\n",
      "\n",
      "\n",
      "[-0.27897714 -0.09267282]\n",
      "[-1.3948857  -0.46336412]\n",
      "\n",
      "\n",
      "[-0.10246983 -0.07363655]\n",
      "[-0.51234916 -0.36818273]\n",
      "\n",
      "\n",
      "[-0.28481033 -0.06311875]\n",
      "[-1.42405163 -0.31559376]\n",
      "\n",
      "\n",
      "[-0.31801872  0.01362213]\n",
      "[-1.5900936   0.06811067]\n",
      "\n",
      "\n",
      "[-0.096841   0.0027201]\n",
      "[-0.484205    0.01360051]\n",
      "\n",
      "\n",
      "[-0.17098932  0.15310175]\n",
      "[-0.85494661  0.76550875]\n",
      "\n",
      "\n",
      "[-0.45556669 -0.12558915]\n",
      "[-2.27783345 -0.62794577]\n",
      "\n",
      "\n",
      "[-0.07467718 -0.09980991]\n",
      "[-0.37338592 -0.49904954]\n",
      "\n",
      "\n",
      "[-0.08667787 -0.00770643]\n",
      "[-0.43338936 -0.03853215]\n",
      "\n",
      "\n",
      "[-0.17967477  0.19269749]\n",
      "[-0.89837383  0.96348744]\n",
      "\n",
      "\n",
      "[-0.11373097  0.04868323]\n",
      "[-0.56865487  0.24341616]\n",
      "\n",
      "\n",
      "[-0.06272287  0.12611606]\n",
      "[-0.31361435  0.6305803 ]\n",
      "\n",
      "\n",
      "[-0.08467258  0.28782281]\n",
      "[-0.42336291  1.43911406]\n",
      "\n",
      "\n",
      "[0.09378645 0.50048985]\n",
      "[0.46893224 2.50244923]\n",
      "\n",
      "\n",
      "[0.31758405 0.10652533]\n",
      "[1.58792026 0.53262666]\n",
      "\n",
      "\n",
      "[-0.32865609  0.04478299]\n",
      "[-1.64328046  0.22391496]\n",
      "\n",
      "\n",
      "[-0.24937855  0.0136279 ]\n",
      "[-1.24689274  0.06813951]\n",
      "\n",
      "\n",
      "[-0.16514941 -0.13384661]\n",
      "[-0.82574703 -0.66923305]\n",
      "\n",
      "\n",
      "[-0.14836271 -0.08947992]\n",
      "[-0.74181354 -0.44739962]\n",
      "\n",
      "\n",
      "[-0.19939556 -0.249194  ]\n",
      "[-0.99697778 -1.24597002]\n",
      "\n",
      "\n",
      "[-0.29524484 -0.16697132]\n",
      "[-1.47622421 -0.83485659]\n",
      "\n",
      "\n",
      "[-0.18561836  0.11204723]\n",
      "[-0.92809179  0.56023613]\n",
      "\n",
      "\n",
      "[-0.06958228 -0.15655124]\n",
      "[-0.34791138 -0.78275621]\n",
      "\n",
      "\n",
      "[-0.01228452  0.29438762]\n",
      "[-0.06142259  1.47193808]\n",
      "\n",
      "\n",
      "[-0.10609562  0.06983829]\n",
      "[-0.5304781   0.34919143]\n",
      "\n",
      "\n",
      "[-0.04010234  0.22853066]\n",
      "[-0.2005117   1.14265328]\n",
      "\n",
      "\n",
      "[ 0.0401358  -0.15672451]\n",
      "[ 0.200679   -0.78362256]\n",
      "\n",
      "\n",
      "[-0.00207628  0.14961441]\n",
      "[-0.0103814   0.74807207]\n",
      "\n",
      "\n",
      "[0.09294521 0.17640792]\n",
      "[0.46472605 0.88203961]\n",
      "\n",
      "\n",
      "[0.4523598 0.0122249]\n",
      "[2.261799   0.06112449]\n",
      "\n",
      "\n",
      "[-0.15731045 -0.14522839]\n",
      "[-0.78655226 -0.72614197]\n",
      "\n",
      "\n",
      "[-0.05455827 -0.05660325]\n",
      "[-0.27279133 -0.28301623]\n",
      "\n",
      "\n",
      "[ 0.22998361 -0.20390726]\n",
      "[ 1.14991806 -1.01953629]\n",
      "\n",
      "\n",
      "[-0.37840609  0.17580574]\n",
      "[-1.89203046  0.8790287 ]\n",
      "\n",
      "\n",
      "[-0.32005595  0.04269803]\n",
      "[-1.60027973  0.21349013]\n",
      "\n",
      "\n",
      "[-0.2703723   0.15660893]\n",
      "[-1.35186151  0.78304466]\n",
      "\n",
      "\n",
      "[-0.22344209 -0.18472881]\n",
      "[-1.11721046 -0.92364404]\n",
      "\n",
      "\n",
      "[0.08378178 0.22371464]\n",
      "[0.41890888 1.11857321]\n",
      "\n",
      "\n",
      "[0.28790122 0.29741471]\n",
      "[1.43950609 1.48707353]\n",
      "\n",
      "\n",
      "[0.03650085 0.08557094]\n",
      "[0.18250423 0.42785468]\n",
      "\n",
      "\n",
      "[-0.01464113  0.25238319]\n",
      "[-0.07320565  1.26191597]\n",
      "\n",
      "\n",
      "[-0.03172464  0.01859873]\n",
      "[-0.15862322  0.09299364]\n",
      "\n",
      "\n",
      "[-0.09472809  0.06154169]\n",
      "[-0.47364046  0.30770846]\n",
      "\n",
      "\n",
      "[0.15392291 0.08607872]\n",
      "[0.76961454 0.43039359]\n",
      "\n",
      "\n",
      "[0.11034826 0.16656859]\n",
      "[0.55174131 0.83284296]\n",
      "\n",
      "\n",
      "[-0.05905397  0.11511534]\n",
      "[-0.29526984  0.57557672]\n",
      "\n",
      "\n",
      "[0.00158646 0.06839797]\n",
      "[0.0079323  0.34198985]\n",
      "\n",
      "\n",
      "[ 0.03460029 -0.12536669]\n",
      "[ 0.17300144 -0.62683344]\n",
      "\n",
      "\n",
      "[-0.0397698  -0.04118368]\n",
      "[-0.19884899 -0.2059184 ]\n",
      "\n",
      "\n",
      "[-0.11862591 -0.28887906]\n",
      "[-0.59312955 -1.4443953 ]\n",
      "\n",
      "\n",
      "[-0.19953492 -0.03862676]\n",
      "[-0.99767461 -0.1931338 ]\n",
      "\n",
      "\n",
      "[-0.20842931 -0.23698697]\n",
      "[-1.04214653 -1.18493486]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize the vectors around each chosen word so every chosen word becomes (0,0)\n",
    "for t in topic_vecs:\n",
    "    rel_data = topic_vecs[t]\n",
    "    for d in rel_data:\n",
    "        vecs = rel_data[d]\n",
    "        \n",
    "        # this is what you'll normalize around\n",
    "        topic_vec = vecs['vec']\n",
    "        zero_vec = np.zeros(len(topic_vec))\n",
    "        \n",
    "        transform_vec = zero_vec - topic_vec\n",
    "        \n",
    "        closest = vecs['sim'] \n",
    "        for word in closest:\n",
    "            \n",
    "            # grab the original vector\n",
    "            og_sim_vec = closest[word]\n",
    "            \n",
    "            # transform the vector so it's centered around the current topic\n",
    "            transformed_sim_vec = og_sim_vec + transform_vec\n",
    "            \n",
    "            print(transformed_sim_vec)\n",
    "            \n",
    "            transformed_sim_vec *= 5\n",
    "            \n",
    "            print(transformed_sim_vec)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            # reassign the transformed vector\n",
    "            closest[word] = list(transformed_sim_vec)\n",
    "        \n",
    "        # finally, reassign the original vector...\n",
    "        vecs['vec'] = list(zero_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ot': {'vec': [0.0, 0.0],\n",
       "  'sim': {'interpretation': [-0.5288959294557571, -0.26165534975007176],\n",
       "   'doubt': [0.5937421694397926, 0.6425301800481975],\n",
       "   'vision': [0.2554737776517868, -0.456859462428838],\n",
       "   'sorcerer': [-0.5969307571649551, 0.402609899174422],\n",
       "   'believe': [-1.1669497936964035, -0.5727749620564282]}},\n",
       " 'nt': {'vec': [0.0, 0.0],\n",
       "  'sim': {'joseph': [-0.842830203473568, -0.10231480002403259],\n",
       "   'secretly': [-0.8009632676839828, 0.26591241359710693],\n",
       "   'daughter': [-0.9431486576795578, 0.6957992911338806],\n",
       "   'young': [-0.29383212327957153, 1.6619715094566345],\n",
       "   'appeared': [-0.08181676268577576, 0.2856495976448059]}},\n",
       " 'q': {'vec': [0.0, 0.0],\n",
       "  'sim': {'withered': [-0.4470434784889221, 0.4326671361923218],\n",
       "   'seven': [-0.9762729704380035, 1.3724810257554054],\n",
       "   'diligently': [0.6552198529243469, 0.12073054909706116],\n",
       "   'figure': [0.3182573616504669, 0.9817350655794144],\n",
       "   'clay': [-0.4415428638458252, 1.3858495652675629]}}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_vecs['dream']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "word {\n",
    "    \"ot\": [],\n",
    "    \"nt\"\n",
    "}\n",
    "\"\"\"\n",
    "polygon_data = {}\n",
    "\n",
    "for t in topic_vecs:\n",
    "    \n",
    "    rel_data = topic_vecs[t]\n",
    "    polygon_data[t] = {}\n",
    "    \n",
    "    for d in rel_data:\n",
    "        vecs = rel_data[d]\n",
    "        \n",
    "        ref_vec = vecs['vec']\n",
    "        sim_vecs = list(vecs['sim'].items())\n",
    "                \n",
    "        polygon_data[t][d] = {\n",
    "            \"poly\": g.Polygon([(0,0)] + [(v[1][0], v[1][1]) for v in sim_vecs]),\n",
    "            \"words\": [t] + [v[0] for v in sim_vecs]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating polygon / DXF data\n",
      "faith\n",
      "sight\n",
      "birth\n",
      "prophet\n",
      "prophecy\n",
      "trust\n",
      "remember\n",
      "experience\n",
      "dream\n",
      "god\n",
      "heaven\n",
      "hell\n",
      "love\n",
      "hate\n",
      "free\n",
      "vision\n",
      "believe\n",
      "light\n",
      "forget\n",
      "darkness\n",
      "peace\n",
      "war\n",
      "life\n",
      "death\n",
      "man\n",
      "woman\n",
      "child\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "print (\"Creating polygon / DXF data\")\n",
    "\n",
    "for topic in polygon_data:\n",
    "    \n",
    "    print(topic)\n",
    "    all_meta = polygon_data[topic]\n",
    "    \n",
    "    for rel in all_meta:\n",
    "        \n",
    "        # grab the specific religions meta data\n",
    "        rel_meta = all_meta[rel]\n",
    "                \n",
    "        # all the words\n",
    "        rel_words = rel_meta['words']\n",
    "        \n",
    "        # grab the convex hull and regular polygon data \n",
    "        rel_convex = list(rel_meta['poly'].convex_hull.exterior.coords)\n",
    "        rel_poly = list(rel_meta['poly'].exterior.coords)\n",
    "        \n",
    "        #####################################\n",
    "        # DXF \n",
    "        #####################################\n",
    "        \n",
    "        # CONFIGURE THE FILE PATHS FOR SAVING\n",
    "        folder_path = '../data/analyzed/dxf/religion-specific/{}'.format(topic)\n",
    "\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.mkdir(folder_path)\n",
    "            \n",
    "        # ADD THE OUTSIDE POLYGON\n",
    "        drawing = dxf.drawing('../data/analyzed/dxf/religion-specific/{}/{}-{}.dxf'.format(topic,topic,rel))\n",
    "        outline = dxf.polyline(linetype='CONTINUOUS', layer = 'OUTLINE')\n",
    "        outline.add_vertices(rel_convex)\n",
    "        \n",
    "        text_layer = dxf.layer('TEXT')\n",
    "        drawing.layers.add(text_layer)\n",
    "\n",
    "        # ADD THE TEXT AND CONNECTING LINES\n",
    "        for p in zip(rel_words, rel_poly):\n",
    "            t = dxf.text(p[0], p[1], height=0.11, rotation=0, layer = 'TEXT')\n",
    "            line = dxf.line(p[1], (0.0, 0.0))\n",
    "            line['linetype'] = \"DASHED2\"\n",
    "            \n",
    "            drawing.add(line)\n",
    "            drawing.add(t)\n",
    "               \n",
    "        # ADD THE ORIGIN POINT\n",
    "#         origin_word = dxf.text(topic, (0.0,0.0), height=0.025, rotation=0, layer = 'TEXT')\n",
    "#         drawing.add(origin_word)\n",
    "        origin = dxf.circle(0.025, (0.0, 0.0))\n",
    "        drawing.add(origin)\n",
    "        \n",
    "        # Close the lines\n",
    "        outline.close()\n",
    "        drawing.add(outline)\n",
    "        drawing.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating polygon / DXF data\n",
      "faith\n",
      "ot-faith\n",
      "nt-faith\n",
      "q-faith\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c8be748>\n",
      "sight\n",
      "ot-sight\n",
      "nt-sight\n",
      "q-sight\n",
      "<dxfwrite.sections.Blocks object at 0x1a2e711a20>\n",
      "birth\n",
      "ot-birth\n",
      "nt-birth\n",
      "q-birth\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c61de48>\n",
      "prophet\n",
      "ot-prophet\n",
      "nt-prophet\n",
      "q-prophet\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c66b630>\n",
      "prophecy\n",
      "ot-prophecy\n",
      "nt-prophecy\n",
      "q-prophecy\n",
      "<dxfwrite.sections.Blocks object at 0x1a2e6e7470>\n",
      "trust\n",
      "ot-trust\n",
      "nt-trust\n",
      "q-trust\n",
      "<dxfwrite.sections.Blocks object at 0x1a2e6e7fd0>\n",
      "remember\n",
      "ot-remember\n",
      "nt-remember\n",
      "q-remember\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c8bed30>\n",
      "experience\n",
      "ot-experience\n",
      "nt-experience\n",
      "q-experience\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c674be0>\n",
      "dream\n",
      "ot-dream\n",
      "nt-dream\n",
      "q-dream\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c66deb8>\n",
      "god\n",
      "ot-god\n",
      "nt-god\n",
      "q-god\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c8be470>\n",
      "heaven\n",
      "ot-heaven\n",
      "nt-heaven\n",
      "q-heaven\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c6016a0>\n",
      "hell\n",
      "ot-hell\n",
      "nt-hell\n",
      "q-hell\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c674cc0>\n",
      "love\n",
      "ot-love\n",
      "nt-love\n",
      "q-love\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c61dcc0>\n",
      "hate\n",
      "ot-hate\n",
      "nt-hate\n",
      "q-hate\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c66db70>\n",
      "free\n",
      "ot-free\n",
      "nt-free\n",
      "q-free\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c6010b8>\n",
      "vision\n",
      "ot-vision\n",
      "nt-vision\n",
      "q-vision\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c674f98>\n",
      "believe\n",
      "ot-believe\n",
      "nt-believe\n",
      "q-believe\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c981b70>\n",
      "light\n",
      "ot-light\n",
      "nt-light\n",
      "q-light\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c674e10>\n",
      "forget\n",
      "ot-forget\n",
      "nt-forget\n",
      "q-forget\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c963828>\n",
      "darkness\n",
      "ot-darkness\n",
      "nt-darkness\n",
      "q-darkness\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c601128>\n",
      "peace\n",
      "ot-peace\n",
      "nt-peace\n",
      "q-peace\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c981b70>\n",
      "war\n",
      "ot-war\n",
      "nt-war\n",
      "q-war\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c8beef0>\n",
      "life\n",
      "ot-life\n",
      "nt-life\n",
      "q-life\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c9633c8>\n",
      "death\n",
      "ot-death\n",
      "nt-death\n",
      "q-death\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c6014a8>\n",
      "man\n",
      "ot-man\n",
      "nt-man\n",
      "q-man\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c464dd8>\n",
      "woman\n",
      "ot-woman\n",
      "nt-woman\n",
      "q-woman\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c6678d0>\n",
      "child\n",
      "ot-child\n",
      "nt-child\n",
      "q-child\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c981438>\n",
      "eat\n",
      "ot-eat\n",
      "nt-eat\n",
      "q-eat\n",
      "<dxfwrite.sections.Blocks object at 0x1a2c6019b0>\n"
     ]
    }
   ],
   "source": [
    "print (\"Creating polygon / DXF data\")\n",
    "from dxfwrite import DXFList\n",
    "for topic in polygon_data:\n",
    "    \n",
    "    print(topic)\n",
    "    all_meta = polygon_data[topic]\n",
    "    \n",
    "    # CONFIGURE THE FILE PATHS FOR SAVING\n",
    "    folder_path = '../data/analyzed/dxf/religion-specific-joined-blocks/'\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "        \n",
    "    drawing = dxf.drawing('../data/analyzed/dxf/religion-specific-joined-blocks/{}.dxf'.format(topic))\n",
    "\n",
    "    for rel in all_meta:\n",
    "        \n",
    "        # MAIN GROUPING\n",
    "        # ADD ENTITIES TO THE BLOCK\n",
    "        print(\"{}-{}\".format(rel,topic))\n",
    "        block = dxf.block(name=\"{}-{}\".format(rel,topic))\n",
    "        entities = DXFList()\n",
    "#         entities.name = \"{}-{}\".format(rel,topic)\n",
    "        \n",
    "#         rel_layer = dxf.layer(rel)\n",
    "#         tl = dxf.layer('text')\n",
    "#         drawing.layers.add(rel_layer)\n",
    "#         rel_layer.add(tl)\n",
    "     \n",
    "        \n",
    "        # grab the specific religions meta data\n",
    "        rel_meta = all_meta[rel]\n",
    "                \n",
    "        # all the words\n",
    "        rel_words = rel_meta['words']\n",
    "        \n",
    "        # grab the convex hull and regular polygon data \n",
    "        rel_convex = list(rel_meta['poly'].convex_hull.exterior.coords)\n",
    "        rel_poly = list(rel_meta['poly'].exterior.coords)\n",
    "        \n",
    "        #####################################\n",
    "        # DXF \n",
    "        #####################################\n",
    "            \n",
    "        # ADD THE OUTSIDE POLYGON\n",
    "        outline = dxf.polyline(linetype='CONTINUOUS', layer = 'OUTLINE_{}'.format(rel))\n",
    "        outline.add_vertices(rel_convex)\n",
    "        \n",
    "        text_layer = dxf.layer('TEXT_{}'.format(rel))\n",
    "        drawing.layers.add(text_layer)\n",
    "\n",
    "        # ADD THE TEXT AND CONNECTING LINES\n",
    "        for p in zip(rel_words, rel_poly):\n",
    "            t = dxf.text(p[0], p[1], height=0.11, rotation=0, layer = \"TEXT_{}\".format(rel))\n",
    "            \n",
    "            line = dxf.line(p[1], (0.0, 0.0))\n",
    "            line['linetype'] = \"DASHED2\"\n",
    "            line['layer'] = \"LINES_{}\".format(rel)\n",
    "            \n",
    "            entities.append(line)\n",
    "            entities.append(t)\n",
    "               \n",
    "        # ADD THE ORIGIN POINT\n",
    "#         origin_word = dxf.text(topic, (0.0,0.0), height=0.025, rotation=0, layer = 'TEXT')\n",
    "#         drawing.add(origin_word)\n",
    "        origin = dxf.circle(0.025, (0.0, 0.0), layer = \"ORIGIN_{}\".format(rel))\n",
    "        entities.append(origin)\n",
    "        \n",
    "        # Close the lines\n",
    "        outline.close()\n",
    "        drawing.add(outline)\n",
    "        \n",
    "        block.add(entities)\n",
    "        drawing.blocks.add(block)\n",
    "        \n",
    "        # create a block-reference\n",
    "        blockref = dxf.insert(blockname=\"{}-{}\".format(rel,topic), insert=(0, 0))\n",
    "        # add block-reference to drawing\n",
    "        drawing.add(blockref)\n",
    "        \n",
    "#         print(block.name)\n",
    "\n",
    "    \n",
    "    print(drawing.blocks)\n",
    "    \n",
    "    drawing.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
