{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from itertools import chain, combinations\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "Getting the data from the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_df = pd.read_csv(\"../data/old-testament-verses.csv\")\n",
    "nt_df = pd.read_csv(\"../data/new-testament-verses.csv\")\n",
    "\n",
    "#rename from AyahText to VerseText\n",
    "q_df = pd.read_csv(\"../data/quran-verses.csv\")\n",
    "q_df.columns = ['DatabaseID', 'SuraID', 'VerseID', 'VerseText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines(df):\n",
    "    return list(df[\"VerseText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stops = [w for w in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the stop words the most common words...?\n",
    "new_stops = ['hath', \"'s\",'let','behold', 'went','o','hast','thine','like','thing','things','quot','and', 'in', 'thou', 'thee', 'thy', 'unto', 'ye', 'said', 'saith', 'shall', 'shalt', 'yea', 'thereof']\n",
    "all_stops += new_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def tokenize(text):\n",
    "    return [t.lower() for t in word_tokenize(text) if t not in punctuation and t.lower() not in all_stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab preparation\n",
    "\n",
    "Constructing a holistic Doc2Vec model for all of the texts put together. Basically, this is just Word2Vec but tagging each word with which religion it belongs to (according to a probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(df):\n",
    "    vocab = []\n",
    "    for l in get_lines(df):\n",
    "        tokens = tokenize(l)\n",
    "        vocab += tokens\n",
    "    return set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shared(sets):\n",
    "    # Finds the intersection of all the input sets\n",
    "    return reduce((lambda set1,set2: set1&set2), sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sym_diffs(sets):\n",
    "    # Finds the symm etric difference of a list of sets\n",
    "    return reduce((lambda set1, set2: set1.symmetric_difference(set2)), sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_uniques(sets):\n",
    "    # Gets the unique elements in each set\n",
    "    # return a dictionary with labels of each\n",
    "    intersection = get_shared(sets)\n",
    "    sym_diffs = get_sym_diffs(sets)\n",
    "    return sym_diffs - intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_vocabs(vocabs_dict):\n",
    "    \n",
    "    all_uniques = get_all_uniques(vocabs_dict.values())\n",
    "    unique_dict = {}\n",
    "    \n",
    "    for tag, vocab in vocabs_dict.items():\n",
    "        unique_dict[tag] = []\n",
    "        for w in vocab:\n",
    "            if w in all_uniques:\n",
    "                unique_dict[tag].append(w)\n",
    "    \n",
    "    return unique_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing set operations\n",
    "set1 = set([1,2,3,4,5])\n",
    "set2 = set([3,4,5,6,7])\n",
    "set3 = set([5,6,7,8,9])\n",
    "\n",
    "s = get_sym_diffs([set1,set2,set3])\n",
    "i = get_shared([set1,set2,set3])\n",
    "u = get_all_uniques([set1,set2,set3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, find the common set of words.\n",
    "# get vocabularies of each\n",
    "ot_vocab = get_vocab(ot_df)\n",
    "nt_vocab = get_vocab(nt_df)\n",
    "q_vocab = get_vocab(q_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = {\n",
    "    \"ot\": ot_vocab,\n",
    "    \"nt\": nt_vocab,\n",
    "    \"q\": q_vocab\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vocabs = get_unique_vocabs(vocabs)\n",
    "shared_vocab = get_shared(vocabs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(vocab, tag):\n",
    "    d = {}\n",
    "    for w in vocab:\n",
    "        d[w] = tag\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(dicts): \n",
    "    super_dict = {}\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            super_dict[k] = v\n",
    "    return super_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def have_common(l1,l2):\n",
    "    result = False\n",
    "    for x in l1: \n",
    "        for y in l2: \n",
    "            if x == y:\n",
    "                print(\"Got same:\", x, y)\n",
    "                result = True\n",
    "                return result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries for references before building the docs\n",
    "ot_dict = build_dict(unique_vocabs['ot'], 'ot')\n",
    "nt_dict = build_dict(unique_vocabs['nt'], 'nt')\n",
    "q_dict = build_dict(unique_vocabs['q'], 'q')\n",
    "shared_dict = build_dict(shared_vocab, 's')\n",
    "all_dict = merge([ot_dict, nt_dict, q_dict, shared_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another pathetic Word2Vec attempt.\n",
    "Fuck me up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(l):\n",
    "    lmin = min(l)\n",
    "    lmax = max(l)\n",
    "    return [(v-lmin)/(lmax-lmin) for v in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proportions(df):\n",
    "    # calculates the normalized proportions of each word's occurence in a text\n",
    "    \n",
    "    # get the raw data\n",
    "    verses = list(df[\"VerseText\"])\n",
    "    docs = [tokenize(v) for v in verses]\n",
    "    \n",
    "    # combine into a 1d list of words\n",
    "    flat = sum(docs, [])\n",
    "    \n",
    "    # First get the counts of all of the words\n",
    "    counts = Counter(flat)\n",
    "    \n",
    "    props = {}\n",
    "    total_count = sum(counts.values())\n",
    "    \n",
    "    # calculate the proportions of all of the words relative to the total count\n",
    "    for word in counts:\n",
    "        word_count = counts[word]\n",
    "        prop = word_count / total_count\n",
    "        props[word] = prop\n",
    "    \n",
    "    # normalize between 0 and 1\n",
    "    prop_vals = list(props.values())\n",
    "    prop_scaled = normalize(prop_vals)\n",
    "    \n",
    "    # reassign the values\n",
    "    for i, w in enumerate(props):\n",
    "        props[w] = prop_scaled[i]\n",
    "        \n",
    "    return props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(d, rev):\n",
    "    return sorted(d.items(), key=lambda kv: kv[1], reverse=rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_props = get_proportions(ot_df)\n",
    "nt_props = get_proportions(nt_df)\n",
    "q_props = get_proportions(q_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_ot = sort_dict(ot_props, True)\n",
    "sorted_nt = sort_dict(nt_props, True)\n",
    "sorted_q = sort_dict(q_props, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will store all of the words and their relative proportions\n",
    "global_prop_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do u need\n",
    "\"\"\"\n",
    "now you want to format each word in the vocabulary with a proportion\n",
    "so instead of just\n",
    "god\n",
    "or\n",
    "god_s\n",
    "it becomes fully descriptive:\n",
    "\n",
    "god_{'ot':0.42,'nt':'0.32','q':'0.01'}\n",
    "this way when you download the data u can parse the vectors\n",
    "after the underscore to access the meta data\n",
    "\n",
    "so now u wanna:\n",
    "\n",
    "go thrrough each of the dictionaries, word by word\n",
    "check if it is in shared, q, ot, or nt\n",
    "construct a  probability dictionary showing the distrirbution\n",
    "\n",
    "then go through each of the dfs and tag each of the words with the distribution\n",
    "\"\"\"\n",
    "\n",
    "all_vocab = set(merge([ot_props, nt_props,q_props]))\n",
    "\n",
    "# first, populate the global dict with all of the words\n",
    "for v in all_vocab:\n",
    "    global_prop_dict[v] = {\n",
    "        'ot': 0,\n",
    "        'nt': 0,\n",
    "        'q': 0\n",
    "    }\n",
    "\n",
    "# now, go through each of the proportion dictionaries and\n",
    "# fill in the relevant proportions for each word from each text\n",
    "round_factor = 10\n",
    "\n",
    "for p in ot_props:\n",
    "    global_prop_dict[p]['ot'] = round(ot_props[p], round_factor)\n",
    "    \n",
    "for p in nt_props:\n",
    "    global_prop_dict[p]['nt'] = round(nt_props[p], round_factor)\n",
    "\n",
    "for p in q_props:\n",
    "    global_prop_dict[p]['q'] = round(q_props[p], round_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe instead of tagging just by shared, tag it with a proportion of how frequently that word occurs in each text?\n",
    "# out of all the occurences of love across the nt, ot, and q\n",
    "# need to make it relative\n",
    "# so, for ex, love would be appended by: \n",
    "# love_{ot:0.75,nt:0.5,q:02}\n",
    "# meaning the vector for love appears mostly in the old testament \n",
    "def tag_word(word, tag_dict):\n",
    "    return word + \"_{}\".format(str(tag_dict[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_docs(df, tag_dict):\n",
    "    ref_keys = tag_dict.keys()\n",
    "    verses = list(df[\"VerseText\"])\n",
    "    docs = [tokenize(v) for v in verses]\n",
    "    # do you even need to do this? just do it after.\n",
    "#     docs = [[tag_word(w, tag_dict) for w in d if w in ref_keys] for d in docs]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_dfs(dfs):\n",
    "    # Ignore the index to avoid having conflicting row numbers when accessing the df\n",
    "    return pd.concat(dfs, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "master_df = concat_dfs([ot_df, nt_df, q_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = get_word2vec_docs(master_df, global_prop_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global hyper parameters\n",
    "hp = {\n",
    "    \"size\": 150, # size of the one-hot-encoded word vectors\n",
    "    \"window\": 20, # context size\n",
    "    \"min_count\": 2,\n",
    "    \"workers\": 4,\n",
    "    \"iter\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO : PROGRESS: at sentence #10000, processed 97905 words, keeping 6371 word types\n",
      "INFO : PROGRESS: at sentence #20000, processed 182563 words, keeping 9736 word types\n",
      "INFO : PROGRESS: at sentence #30000, processed 265171 words, keeping 12054 word types\n",
      "INFO : collected 15518 word types from a corpus of 332240 raw words and 37338 sentences\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : min_count=2 retains 10130 unique words (65% of original 15518, drops 5388)\n",
      "INFO : min_count=2 leaves 326852 word corpus (98% of original 332240, drops 5388)\n",
      "INFO : deleting the raw counts dictionary of 15518 items\n",
      "INFO : sample=0.001 downsamples 35 most-common words\n",
      "INFO : downsampling leaves estimated 298926 word corpus (91.5% of prior 326852)\n",
      "INFO : estimated required memory for 10130 words and 150 dimensions: 17221000 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : training model with 4 workers on 10130 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=20\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 332240 raw words (299042 effective words) took 0.2s, 1684871 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 332240 raw words (298723 effective words) took 0.2s, 1420428 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 332240 raw words (299041 effective words) took 0.2s, 1637352 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 332240 raw words (298800 effective words) took 0.2s, 1471990 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 332240 raw words (298935 effective words) took 0.2s, 1729454 effective words/s\n",
      "INFO : training on a 1661200 raw words (1494541 effective words) took 1.0s, 1498878 effective words/s\n",
      "WARNING : Effective 'alpha' higher than previous training cycles\n",
      "INFO : training model with 4 workers on 10130 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=20\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 332240 raw words (298884 effective words) took 0.2s, 1617512 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 332240 raw words (298983 effective words) took 0.2s, 1651080 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 332240 raw words (298910 effective words) took 0.2s, 1771622 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 332240 raw words (298959 effective words) took 0.2s, 1460939 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 332240 raw words (299053 effective words) took 0.2s, 1620879 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 6 : training on 332240 raw words (298840 effective words) took 0.2s, 1712418 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 7 : training on 332240 raw words (298898 effective words) took 0.2s, 1645663 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 8 : training on 332240 raw words (299176 effective words) took 0.2s, 1490651 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 9 : training on 332240 raw words (298961 effective words) took 0.2s, 1720322 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 10 : training on 332240 raw words (298804 effective words) took 0.2s, 1743812 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 11 : training on 332240 raw words (299031 effective words) took 0.2s, 1866706 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 12 : training on 332240 raw words (298939 effective words) took 0.2s, 1912079 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 13 : training on 332240 raw words (298919 effective words) took 0.2s, 1650346 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 14 : training on 332240 raw words (298971 effective words) took 0.2s, 1521361 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 15 : training on 332240 raw words (299025 effective words) took 0.2s, 1492353 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 16 : training on 332240 raw words (298967 effective words) took 0.2s, 1455787 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 17 : training on 332240 raw words (299046 effective words) took 0.2s, 1593360 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 18 : training on 332240 raw words (298850 effective words) took 0.2s, 1694295 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 19 : training on 332240 raw words (299122 effective words) took 0.2s, 1718994 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 20 : training on 332240 raw words (298926 effective words) took 0.2s, 1724452 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 21 : training on 332240 raw words (299255 effective words) took 0.3s, 1185814 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 22 : training on 332240 raw words (298866 effective words) took 0.2s, 1259734 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 23 : training on 332240 raw words (298681 effective words) took 0.2s, 1525346 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 24 : training on 332240 raw words (298860 effective words) took 0.2s, 1528463 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 25 : training on 332240 raw words (298939 effective words) took 0.2s, 1534306 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 26 : training on 332240 raw words (299097 effective words) took 0.2s, 1534593 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 27 : training on 332240 raw words (298875 effective words) took 0.2s, 1513998 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 28 : training on 332240 raw words (299076 effective words) took 0.2s, 1718320 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 29 : training on 332240 raw words (299011 effective words) took 0.2s, 1562014 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 30 : training on 332240 raw words (298610 effective words) took 0.2s, 1565652 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 31 : training on 332240 raw words (298669 effective words) took 0.2s, 1290328 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 32 : training on 332240 raw words (298796 effective words) took 0.2s, 1463881 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 33 : training on 332240 raw words (298873 effective words) took 0.2s, 1629112 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 34 : training on 332240 raw words (299113 effective words) took 0.2s, 1515728 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 35 : training on 332240 raw words (298843 effective words) took 0.2s, 1563708 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 36 : training on 332240 raw words (298951 effective words) took 0.2s, 1622922 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 37 : training on 332240 raw words (298960 effective words) took 0.2s, 1222086 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 38 : training on 332240 raw words (298800 effective words) took 0.2s, 1729012 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 39 : training on 332240 raw words (298965 effective words) took 0.2s, 1561392 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 40 : training on 332240 raw words (299097 effective words) took 0.2s, 1622340 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 41 : training on 332240 raw words (298877 effective words) took 0.2s, 1569951 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 42 : training on 332240 raw words (299112 effective words) took 0.2s, 1638846 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 43 : training on 332240 raw words (299053 effective words) took 0.2s, 1680823 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 44 : training on 332240 raw words (298901 effective words) took 0.2s, 1541120 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 45 : training on 332240 raw words (299018 effective words) took 0.2s, 1544169 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 46 : training on 332240 raw words (298873 effective words) took 0.2s, 1502534 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 47 : training on 332240 raw words (299346 effective words) took 0.2s, 1637742 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 48 : training on 332240 raw words (298810 effective words) took 0.2s, 1412252 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 49 : training on 332240 raw words (298807 effective words) took 0.2s, 1451028 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 50 : training on 332240 raw words (298995 effective words) took 0.2s, 1515536 effective words/s\n",
      "INFO : training on a 16612000 raw words (14947393 effective words) took 10.1s, 1479815 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14947393, 16612000)"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model = gensim.models.Word2Vec(\n",
    "        all_docs,\n",
    "        size=hp[\"size\"],\n",
    "        window=hp[\"window\"],\n",
    "        min_count=hp[\"min_count\"],\n",
    "        workers=hp[\"workers\"])\n",
    "\n",
    "w2v_model.train(all_docs, total_examples=len(all_docs), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('air', 0.3754453659057617) {'ot': 0.002902557, 'nt': 0.011687363, 'q': 0.0006990563}\n",
      "('vials', 0.355421781539917) {'ot': 0, 'nt': 0.0029218408, 'q': 0}\n",
      "('sheet', 0.34510380029678345) {'ot': 0, 'nt': 0.0007304602, 'q': 0}\n",
      "('heavens', 0.3438462018966675) {'ot': 0.0156185211, 'nt': 0.0131482834, 'q': 0.0695560993}\n",
      "('god', 0.3424279987812042) {'ot': 0.4281962681, 'nt': 1.0, 'q': 0.0297098916}\n",
      "('passions', 0.33372625708580017) {'ot': 0, 'nt': 0.0007304602, 'q': 0}\n",
      "('sitteth', 0.313865065574646) {'ot': 0.003593642, 'nt': 0.0102264427, 'q': 0}\n",
      "('quaked', 0.2997109293937683) {'ot': 0.000138217, 'nt': 0, 'q': 0}\n",
      "('great', 0.28947335481643677) {'ot': 0.0977194195, 'nt': 0.184806428, 'q': 0.0262146103}\n",
      "('dew', 0.2748190760612488) {'ot': 0.004975812, 'nt': 0, 'q': 0}\n"
     ]
    }
   ],
   "source": [
    "# Cool. now we have our model with all of the words tagged with their relative proportions in each text.\n",
    "# Now we want to go through the shared vocabulary list, count up the most common ones, and then use those\n",
    "# as the topics for the visualization.\n",
    "\n",
    "pos = 'heaven'\n",
    "\n",
    "for w in w2v_model.wv.most_similar(positive=[pos]):\n",
    "    print(w, global_prop_dict[w[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the most common topics from the shared list\n",
    "# You want to not choose the most common shared vocab (because it can be over powered by certain texts)\n",
    "# You want to choose the highest ranked shared words by how close their proportions are!\n",
    "\n",
    "top_shared_topics = {}\n",
    "\n",
    "# fill up the eqs array\n",
    "for s in shared_vocab:\n",
    "    # for each distribution, 1) calculate the avg\n",
    "    # your metric for comparison is then the sum of the squared differences from the avg\n",
    "    distribution = global_prop_dict[s]\n",
    "    probs = list(distribution.values())\n",
    "    \n",
    "    # Skip over the distributions without shared values\n",
    "    if 0 in probs:\n",
    "        continue\n",
    "        \n",
    "    avg_dist = np.average(probs)\n",
    "    eq = sum([(avg_dist-p)**2 for p in probs])\n",
    "    \n",
    "    top_shared_topics[s] = eq\n",
    "    \n",
    "# normalize the metrics\n",
    "# eqs = normalize(eqs)\n",
    "\n",
    "eqs_norm = normalize(top_shared_topics.values())\n",
    "\n",
    "# reassignt the normalized values\n",
    "for (i,s) in enumerate(top_shared_topics):\n",
    "    top_shared_topics[s] = eqs_norm[i]\n",
    "\n",
    "sorted_topics = sort_dict(top_shared_topics, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT STEP...\n",
    "# 1. Go through all of the word vectors\n",
    "# 2. Do TSNE (slow, but really accurate) or PCA on them to reduce down to 2 dimensions\n",
    "# 3. Multiply each component by the relative proportion in order to get an overall comparison of the shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "model_X = w2v_model[w2v_model.wv.vocab]\n",
    "model_reduced = tsne.fit_transform(model_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to save the reduced model weights as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for easy reuse\n",
    "model_path = \"../models/combined.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : saving Word2Vec object under ../models/combined.model, separately None\n",
      "INFO : not storing attribute vectors_norm\n",
      "INFO : not storing attribute cum_table\n",
      "INFO : saved ../models/combined.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loading Word2Vec object from ../models/combined.model\n",
      "INFO : loading wv recursively from ../models/combined.model.wv.* with mmap=None\n",
      "INFO : setting ignored attribute vectors_norm to None\n",
      "INFO : loading vocabulary recursively from ../models/combined.model.vocabulary.* with mmap=None\n",
      "INFO : loading trainables recursively from ../models/combined.model.trainables.* with mmap=None\n",
      "INFO : setting ignored attribute cum_table to None\n",
      "INFO : loaded ../models/combined.model\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that you have the reduced vectors... you can analyze!\n",
    "# Going to turn the results into a DataFrame where:\n",
    "# Each row is a word\n",
    "# Each column: word, comp1, comp2, ot_prop, nt_prop, q_prop \n",
    "results = []\n",
    "\n",
    "# also keep a dictionary for easy referencing\n",
    "results_dict = {}\n",
    "\n",
    "for i, word in enumerate(list(w2v_model.wv.vocab)):\n",
    "    \n",
    "    # FOR INTERNAL \n",
    "    ot_prop = global_prop_dict[word]['ot']\n",
    "    nt_prop = global_prop_dict[word]['nt']\n",
    "    q_prop = global_prop_dict[word]['q']\n",
    "    \n",
    "    all_props = [ot_prop, nt_prop, q_prop]\n",
    "    lower, upper = 0.01, 0.99\n",
    "    prop_norm = [lower + (upper - lower) * x for x in all_props]\n",
    "    \n",
    "    if ot_prop == 0 or nt_prop == 0 or q_prop == 0:\n",
    "        continue\n",
    "        \n",
    "    results_dict[word] = {\n",
    "        \"comp1\": model_reduced[i][0],\n",
    "        \"comp2\": model_reduced[i][1],\n",
    "        \"ot_prop\": prop_norm[0],\n",
    "        \"nt_prop\": prop_norm[1],\n",
    "        \"q_prop\": prop_norm[2]\n",
    "    }\n",
    "        \n",
    "    # FOR PANDAS\n",
    "    # store each row as a dictionary\n",
    "    row = {}\n",
    "    \n",
    "    row[\"word\"] = word\n",
    "    \n",
    "    # comp1 and comp2 are the 2 components of the vector\n",
    "    row[\"comp1\"] = model_reduced[i][0]\n",
    "    row[\"comp2\"] = model_reduced[i][1]\n",
    "    \n",
    "    # proportions that each word shows up in each text relative to the length of each text\n",
    "    row[\"ot_prop\"] = prop_norm[0]\n",
    "    row[\"nt_prop\"] = prop_norm[1]\n",
    "    row[\"q_prop\"] = prop_norm[2]\n",
    "    \n",
    "    results.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['word', 'comp1', 'comp2', 'nt_prop', 'ot_prop', 'q_prop']\n",
    "final_df = pd.DataFrame(results)\n",
    "\n",
    "# # Re order the columns\n",
    "final_df = final_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>comp1</th>\n",
       "      <th>comp2</th>\n",
       "      <th>nt_prop</th>\n",
       "      <th>ot_prop</th>\n",
       "      <th>q_prop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beginning</td>\n",
       "      <td>-9.646623</td>\n",
       "      <td>46.818810</td>\n",
       "      <td>0.046508</td>\n",
       "      <td>0.017179</td>\n",
       "      <td>0.011370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>god</td>\n",
       "      <td>44.712818</td>\n",
       "      <td>17.773558</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.429632</td>\n",
       "      <td>0.039116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>created</td>\n",
       "      <td>-31.942595</td>\n",
       "      <td>-3.030641</td>\n",
       "      <td>0.017874</td>\n",
       "      <td>0.014334</td>\n",
       "      <td>0.061723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heaven</td>\n",
       "      <td>46.240665</td>\n",
       "      <td>17.736166</td>\n",
       "      <td>0.192542</td>\n",
       "      <td>0.054158</td>\n",
       "      <td>0.026442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earth</td>\n",
       "      <td>-33.827690</td>\n",
       "      <td>56.489178</td>\n",
       "      <td>0.146728</td>\n",
       "      <td>0.117549</td>\n",
       "      <td>0.150783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>form</td>\n",
       "      <td>-28.772867</td>\n",
       "      <td>-7.134474</td>\n",
       "      <td>0.014295</td>\n",
       "      <td>0.012167</td>\n",
       "      <td>0.012740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>void</td>\n",
       "      <td>-14.909273</td>\n",
       "      <td>28.176413</td>\n",
       "      <td>0.012148</td>\n",
       "      <td>0.012574</td>\n",
       "      <td>0.011713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>darkness</td>\n",
       "      <td>18.829397</td>\n",
       "      <td>54.703842</td>\n",
       "      <td>0.045793</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.019934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>face</td>\n",
       "      <td>24.805086</td>\n",
       "      <td>8.745116</td>\n",
       "      <td>0.050088</td>\n",
       "      <td>0.058492</td>\n",
       "      <td>0.021304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deep</td>\n",
       "      <td>-39.192387</td>\n",
       "      <td>-14.437621</td>\n",
       "      <td>0.015727</td>\n",
       "      <td>0.017450</td>\n",
       "      <td>0.011370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word      comp1      comp2   nt_prop   ot_prop    q_prop\n",
       "0  beginning  -9.646623  46.818810  0.046508  0.017179  0.011370\n",
       "1        god  44.712818  17.773558  0.990000  0.429632  0.039116\n",
       "2    created -31.942595  -3.030641  0.017874  0.014334  0.061723\n",
       "3     heaven  46.240665  17.736166  0.192542  0.054158  0.026442\n",
       "4      earth -33.827690  56.489178  0.146728  0.117549  0.150783\n",
       "5       form -28.772867  -7.134474  0.014295  0.012167  0.012740\n",
       "6       void -14.909273  28.176413  0.012148  0.012574  0.011713\n",
       "7   darkness  18.829397  54.703842  0.045793  0.024900  0.019934\n",
       "8       face  24.805086   8.745116  0.050088  0.058492  0.021304\n",
       "9       deep -39.192387 -14.437621  0.015727  0.017450  0.011370"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../data/analyzed/combined-vectors-props.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More normaliation: take all three of the proportions and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering to only include words with shared values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon construction from Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import geometry as g\n",
    "from dxfwrite import DXFEngine as dxf\n",
    "from dxfwrite.const import CENTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['god', 'servant', 'heaven', 'hell', 'love', 'hate', 'free', 'light', 'darkness', 'peace', 'war', 'life', 'death', 'man', 'woman', 'child', 'eat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scaled_vec(vec,prop):\n",
    "    return list(np.array(vec) * prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Organize the vectors and multiply by the proportions for geometric calculations\n",
    "topic_vectors = {}\n",
    "\n",
    "# unweighted, without scaling the vectors\n",
    "topic_vectors_uw = {}\n",
    "\n",
    "for t in topics:\n",
    "    \n",
    "    similar_tops = w2v_model.most_similar(positive=[t], topn=100)\n",
    "    og = results_dict[t]    \n",
    "    sims = {s[0]:results_dict[s[0]] for s in similar_tops if s[0] in results_dict.keys()}\n",
    "\n",
    "    vecs = [(s[0], [s[1]['comp1'],s[1]['comp2']]) for s in sims.items()]\n",
    "\n",
    "    topic_vectors[t] = {\n",
    "        'ot': [(v[0], calc_scaled_vec(v[1], results_dict[v[0]]['ot_prop'])) for v in vecs],\n",
    "        'nt': [(v[0], calc_scaled_vec(v[1], results_dict[v[0]]['nt_prop'])) for v in vecs],\n",
    "        'q': [(v[0], calc_scaled_vec(v[1], results_dict[v[0]]['q_prop'])) for v in vecs],\n",
    "    }\n",
    "    \n",
    "    topic_vectors_uw[t] = {\n",
    "        'ot': [(v[0], v[1], results_dict[v[0]]['ot_prop']) for v in vecs],\n",
    "        'nt': [(v[0], v[1], results_dict[v[0]]['nt_prop']) for v in vecs],\n",
    "        'q': [(v[0], v[1], results_dict[v[0]]['q_prop']) for v in vecs],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "word {\n",
    "    \"ot\": [],\n",
    "    \"nt\"\n",
    "}\n",
    "\"\"\"\n",
    "polygon_data = {}\n",
    "polygon_data_uw = {}\n",
    "\n",
    "for t in topic_vectors:\n",
    "    vecs = topic_vectors[t]\n",
    "    polygon_data[t] = {}\n",
    "    for r in vecs:\n",
    "        # get the first 10 closest vectors that are non zero to create the polygon\n",
    "        pts = [(v[0], (v[1][0], v[1][1]))for v in vecs[r][:5]]\n",
    "        \n",
    "        topic_words = [p[0] for p in pts]\n",
    "        topic_poly = g.Polygon([p[1] for p in pts])\n",
    "        \n",
    "        polygon_data[t][r] = {}\n",
    "        polygon_data[t][r][\"words\"] = topic_words\n",
    "        polygon_data[t][r][\"poly\"] = topic_poly\n",
    "        \n",
    "for t in topic_vectors_uw:\n",
    "    vecs = topic_vectors_uw[t]\n",
    "    polygon_data_uw[t] = {}\n",
    "    for r in vecs:\n",
    "        # get the first 10 closest vectors that are non zero to create the polygon\n",
    "        pts = [(v[0], (v[1][0], v[1][1]), v[2])for v in vecs[r][:5]]\n",
    "        \n",
    "        topic_words = [p[0] for p in pts]\n",
    "        topic_poly = g.Polygon([p[1] for p in pts])\n",
    "        topic_props = [p[2] for p in pts]\n",
    "        \n",
    "        polygon_data_uw[t][r] = {}\n",
    "        polygon_data_uw[t][r][\"words\"] = topic_words\n",
    "        polygon_data_uw[t][r][\"poly\"] = topic_poly\n",
    "        polygon_data_uw[t][r][\"props\"] = topic_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['darkness', 'shine', 'lights', 'blind', 'dark']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.4688480794429779, 1.3621143102645874),\n",
       " (0.020912475883960724, 0.7100299596786499),\n",
       " (-0.34322747588157654, -0.057937461882829666),\n",
       " (-0.5357349514961243, 0.6746863722801208),\n",
       " (0.5808371305465698, 0.46396303176879883),\n",
       " (0.4688480794429779, 1.3621143102645874)]"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the actual polygon and the convex hull\n",
    "# Draw both, but make the internal lines \n",
    "print(polygon_data['light']['ot']['words'])\n",
    "list(polygon_data['light']['ot']['poly'].exterior.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"-0.592537022382021 -0.11473953276872635 1.1181871727108956 1.5336559139192105\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,1.3041768483817577)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.03067311827838421\" opacity=\"0.6\" d=\"M 0.4688480794429779,1.3621143102645874 L 0.020912475883960724,0.7100299596786499 L -0.5357349514961243,0.6746863722801208 L -0.34322747588157654,-0.057937461882829666 L 0.19356831908226013,0.5684220194816589 L 0.4688480794429779,1.3621143102645874 z\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.polygon.Polygon at 0x1a2dd1eba8>"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygon_data['light']['ot']['poly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"-1.9421451497077942 -0.17377682358026503 2.912252447605133 2.786666511595249\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,2.439112864434719)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.05824504895210266\" opacity=\"0.6\" d=\"M 0.8622460961341858,2.505028486251831 L 0.02393491566181183,0.8126491904258728 L -1.8342839479446411,2.31003475189209 L -0.39049088954925537,-0.06591562181711197 L 0.35503852367401123,1.0425865650177002 L 0.8622460961341858,2.505028486251831 z\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.polygon.Polygon at 0x1a2dd1e390>"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygon_data['light']['nt']['poly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"-0.8340812766551972 -0.1026372642815113 1.2559348332881928 1.2395972187817095\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,1.034322690218687)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.025118696665763857\" opacity=\"0.6\" d=\"M 0.3753374516963959,1.0904438495635986 L 0.016491137444972992,0.5599146485328674 L -0.7875651717185974,0.9918327927589417 L -0.3324674963951111,-0.056121159344911575 L 0.21714875102043152,0.6376670598983765 L 0.3753374516963959,1.0904438495635986 z\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.polygon.Polygon at 0x1a2c453198>"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygon_data['light']['q']['poly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating polygon / DXF data\n",
      "god\n",
      "servant\n",
      "heaven\n",
      "hell\n",
      "love\n",
      "hate\n",
      "free\n",
      "light\n",
      "darkness\n",
      "peace\n",
      "war\n",
      "life\n",
      "death\n",
      "man\n",
      "woman\n",
      "child\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "print (\"Creating polygon / DXF data\")\n",
    "\n",
    "for topic in polygon_data:\n",
    "    \n",
    "    print(topic)\n",
    "    \n",
    "    all_meta = polygon_data[topic]\n",
    "    for rel in all_meta:\n",
    "        \n",
    "        # grab the specific religions meta data\n",
    "        rel_meta = all_meta[rel]\n",
    "        \n",
    "        # all the words\n",
    "        rel_words = rel_meta['words']\n",
    "        \n",
    "        # grab the convex hull and regular polygon data \n",
    "        rel_convex = list(rel_meta['poly'].convex_hull.exterior.coords)\n",
    "        rel_poly = list(rel_meta['poly'].exterior.coords)\n",
    "        \n",
    "        \n",
    "        #####################################\n",
    "        # DXF \n",
    "        #####################################\n",
    "        \n",
    "        # CONFIGURE THE FILE PATHS FOR SAVING\n",
    "        folder_path = '../data/analyzed/dxf/weighted/{}'.format(topic)\n",
    "\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.mkdir(folder_path)\n",
    "            \n",
    "        drawing = dxf.drawing('../data/analyzed/dxf/weighted/{}/{}-{}.dxf'.format(topic,topic,rel))\n",
    "        \n",
    "        line = dxf.polyline(linetype='DOT', layer ='INTERNAL')\n",
    "        outline = dxf.polyline(linetype='CONTINUOUS', layer = 'OUTLINE')\n",
    "        line.add_vertices(rel_poly)\n",
    "        outline.add_vertices(rel_convex)\n",
    "        \n",
    "        text_layer = dxf.layer('TEXT')\n",
    "        drawing.layers.add(text_layer)\n",
    "\n",
    "        # ADD THE TEXT\n",
    "        for p in zip(rel_words, rel_poly):\n",
    "            t = dxf.text(p[0], p[1], height=0.025, rotation=0, layer = 'TEXT')\n",
    "            drawing.add(t)\n",
    "        \n",
    "        # Close the lines\n",
    "        outline.close()\n",
    "        line.close()\n",
    "        \n",
    "        drawing.add(outline)\n",
    "        drawing.add(line)\n",
    "        drawing.save()\n",
    "        \n",
    "        #####################################\n",
    "        # SVG (for actual artwork and illustration!) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating polygon / DXF data with UNWEIGHT\n",
      "god\n",
      "servant\n",
      "heaven\n",
      "hell\n",
      "love\n",
      "hate\n",
      "free\n",
      "light\n",
      "darkness\n",
      "peace\n",
      "war\n",
      "life\n",
      "death\n",
      "man\n",
      "woman\n",
      "child\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "print (\"Creating polygon / DXF data with UNWEIGHT\")\n",
    "\n",
    "for topic in polygon_data_uw:\n",
    "    \n",
    "    print(topic)\n",
    "    \n",
    "    all_meta = polygon_data_uw[topic]\n",
    "    \n",
    "    for rel in all_meta:\n",
    "        \n",
    "        # grab the specific religions meta data\n",
    "        rel_meta = all_meta[rel]\n",
    "        \n",
    "        # all the words\n",
    "        rel_words = rel_meta['words']\n",
    "        \n",
    "        # all the word proportions\n",
    "        rel_props = rel_meta['props']\n",
    "        \n",
    "        # grab the convex hull and regular polygon data \n",
    "        rel_convex = list(rel_meta['poly'].convex_hull.exterior.coords)\n",
    "        rel_poly = list(rel_meta['poly'].exterior.coords)\n",
    "        \n",
    "        \n",
    "        #####################################\n",
    "        # DXF \n",
    "        #####################################\n",
    "        \n",
    "        # CONFIGURE THE FILE PATHS FOR SAVING\n",
    "        folder_path = '../data/analyzed/dxf/unweighted/{}'.format(topic)\n",
    "\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.mkdir(folder_path)\n",
    "            \n",
    "        drawing = dxf.drawing('../data/analyzed/dxf/unweighted/{}/{}-{}.dxf'.format(topic,topic,rel))\n",
    "        \n",
    "        line = dxf.polyline(linetype='DOT', layer ='INTERNAL')\n",
    "        outline = dxf.polyline(linetype='CONTINUOUS', layer = 'OUTLINE')\n",
    "        line.add_vertices(rel_poly)\n",
    "        outline.add_vertices(rel_convex)\n",
    "        \n",
    "        text_layer = dxf.layer('TEXT')\n",
    "        drawing.layers.add(text_layer)\n",
    "        \n",
    "        # CIRCLE PROPS\n",
    "        base_r = 25\n",
    "        \n",
    "        # ADD THE TEXT AND CIRCLES\n",
    "        for p in zip(rel_words, rel_poly, rel_props):\n",
    "            t = dxf.text(p[0], p[1], height=1, rotation=0, layer = 'TEXT')\n",
    "            c = dxf.circle(base_r * p[2], p[1])\n",
    "            \n",
    "            drawing.add(c)\n",
    "            drawing.add(t)\n",
    "        \n",
    "        # Close the lines\n",
    "        outline.close()\n",
    "        line.close()\n",
    "        \n",
    "        drawing.add(outline)\n",
    "        drawing.add(line)\n",
    "        drawing.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec attempt\n",
    "Did not work as expected. Forgot that document IDs had to be unique. Going to a simpler Word2Vec implementation with simple string manipulation for tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(df, tag_dict):\n",
    "    docs = []\n",
    "    # go through each line of the data \n",
    "    for l in get_lines(df):\n",
    "        toks = tokenize(l)\n",
    "        tagged = [TaggedDocument([t], [tag_dict[t]]) for t in toks if t in tag_dict.keys()]\n",
    "        docs += tagged\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_tagged = get_docs(ot_df, all_dict)\n",
    "nt_tagged = get_docs(nt_df, all_dict)\n",
    "q_tagged = get_docs(q_df, all_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all docs now contains every word from all three texts tagged by whether it is unique or shared.\n",
    "all_docs = ot_tagged + nt_tagged + q_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO : PROGRESS: at example #10000, processed 10000 words (273453/s), 1560 word types, 2 tags\n",
      "INFO : PROGRESS: at example #20000, processed 20000 words (307677/s), 2324 word types, 2 tags\n",
      "INFO : PROGRESS: at example #30000, processed 30000 words (301909/s), 2666 word types, 2 tags\n",
      "INFO : PROGRESS: at example #40000, processed 40000 words (300646/s), 3148 word types, 2 tags\n",
      "INFO : PROGRESS: at example #50000, processed 50000 words (285906/s), 3467 word types, 2 tags\n",
      "INFO : PROGRESS: at example #60000, processed 60000 words (292752/s), 4053 word types, 2 tags\n",
      "INFO : PROGRESS: at example #70000, processed 70000 words (301136/s), 4395 word types, 2 tags\n",
      "INFO : PROGRESS: at example #80000, processed 80000 words (308920/s), 4728 word types, 2 tags\n",
      "INFO : PROGRESS: at example #90000, processed 90000 words (302427/s), 5001 word types, 2 tags\n",
      "INFO : PROGRESS: at example #100000, processed 100000 words (318337/s), 5717 word types, 2 tags\n",
      "INFO : PROGRESS: at example #110000, processed 110000 words (329060/s), 6053 word types, 2 tags\n",
      "INFO : PROGRESS: at example #120000, processed 120000 words (329344/s), 6540 word types, 2 tags\n",
      "INFO : PROGRESS: at example #130000, processed 130000 words (297209/s), 6762 word types, 2 tags\n",
      "INFO : PROGRESS: at example #140000, processed 140000 words (292668/s), 7032 word types, 2 tags\n",
      "INFO : PROGRESS: at example #150000, processed 150000 words (305656/s), 7334 word types, 2 tags\n",
      "INFO : PROGRESS: at example #160000, processed 160000 words (302941/s), 7496 word types, 2 tags\n",
      "INFO : PROGRESS: at example #170000, processed 170000 words (318033/s), 7642 word types, 2 tags\n",
      "INFO : PROGRESS: at example #180000, processed 180000 words (315847/s), 7784 word types, 2 tags\n",
      "INFO : PROGRESS: at example #190000, processed 190000 words (283860/s), 7975 word types, 2 tags\n",
      "INFO : PROGRESS: at example #200000, processed 200000 words (282261/s), 8257 word types, 3 tags\n",
      "INFO : PROGRESS: at example #210000, processed 210000 words (293136/s), 8467 word types, 3 tags\n",
      "INFO : PROGRESS: at example #220000, processed 220000 words (296517/s), 8672 word types, 3 tags\n",
      "INFO : PROGRESS: at example #230000, processed 230000 words (291459/s), 9129 word types, 3 tags\n",
      "INFO : PROGRESS: at example #240000, processed 240000 words (297694/s), 9495 word types, 3 tags\n",
      "INFO : PROGRESS: at example #250000, processed 250000 words (287391/s), 10076 word types, 4 tags\n",
      "INFO : PROGRESS: at example #260000, processed 260000 words (288340/s), 10819 word types, 4 tags\n",
      "INFO : PROGRESS: at example #270000, processed 270000 words (278384/s), 11365 word types, 4 tags\n",
      "INFO : PROGRESS: at example #280000, processed 280000 words (285785/s), 11895 word types, 4 tags\n",
      "INFO : PROGRESS: at example #290000, processed 290000 words (290211/s), 12311 word types, 4 tags\n",
      "INFO : PROGRESS: at example #300000, processed 300000 words (293734/s), 12662 word types, 4 tags\n",
      "INFO : collected 12847 word types and 4 unique tags from a corpus of 302893 examples and 302893 words\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : min_count=2 retains 7459 unique words (58% of original 12847, drops 5388)\n",
      "INFO : min_count=2 leaves 297505 word corpus (98% of original 302893, drops 5388)\n",
      "INFO : deleting the raw counts dictionary of 12847 items\n",
      "INFO : sample=0.001 downsamples 50 most-common words\n",
      "INFO : downsampling leaves estimated 260292 word corpus (87.5% of prior 297505)\n",
      "INFO : estimated required memory for 7459 words and 50 dimensions: 6714700 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : training model with 4 workers on 7459 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "INFO : EPOCH 1 - PROGRESS: at 3.30% examples, 16131 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 16.51% examples, 40534 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 29.71% examples, 46726 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 42.92% examples, 50513 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 56.13% examples, 54173 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 69.33% examples, 56714 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 82.54% examples, 58244 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 302893 raw words (563241 effective words) took 8.6s, 65242 effective words/s\n",
      "INFO : EPOCH 2 - PROGRESS: at 3.30% examples, 16753 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 16.51% examples, 42789 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 29.71% examples, 50493 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 42.92% examples, 54855 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 56.13% examples, 57511 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 69.33% examples, 59550 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 82.54% examples, 60930 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 302893 raw words (563140 effective words) took 8.3s, 67822 effective words/s\n",
      "INFO : EPOCH 3 - PROGRESS: at 3.30% examples, 16616 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 16.51% examples, 42959 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 29.71% examples, 51681 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 42.92% examples, 56298 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 56.13% examples, 59351 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 69.33% examples, 61234 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 82.54% examples, 62082 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 302893 raw words (563074 effective words) took 8.2s, 68858 effective words/s\n",
      "INFO : EPOCH 4 - PROGRESS: at 3.30% examples, 17061 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 16.51% examples, 44065 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 29.71% examples, 51881 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 42.92% examples, 54571 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 56.13% examples, 56790 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 69.33% examples, 58279 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 82.54% examples, 59546 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 302893 raw words (563057 effective words) took 8.5s, 66497 effective words/s\n",
      "INFO : EPOCH 5 - PROGRESS: at 3.30% examples, 16568 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 16.51% examples, 42568 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 29.71% examples, 50820 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 42.92% examples, 54881 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 56.13% examples, 57294 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 69.33% examples, 58854 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 82.54% examples, 60455 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 302893 raw words (563031 effective words) took 8.4s, 67307 effective words/s\n",
      "INFO : EPOCH 6 - PROGRESS: at 3.30% examples, 17015 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 6 - PROGRESS: at 16.51% examples, 43158 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 6 - PROGRESS: at 29.71% examples, 50875 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 6 - PROGRESS: at 42.92% examples, 55368 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 6 - PROGRESS: at 56.13% examples, 58049 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 6 - PROGRESS: at 69.33% examples, 59916 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 6 - PROGRESS: at 82.54% examples, 61118 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 6 : training on 302893 raw words (563253 effective words) took 8.2s, 68738 effective words/s\n",
      "INFO : EPOCH 7 - PROGRESS: at 3.30% examples, 17186 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 7 - PROGRESS: at 16.51% examples, 44190 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 7 - PROGRESS: at 29.71% examples, 52633 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 7 - PROGRESS: at 42.92% examples, 57675 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 7 - PROGRESS: at 56.13% examples, 59702 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 7 - PROGRESS: at 69.33% examples, 61309 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 7 - PROGRESS: at 82.54% examples, 62749 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 7 : training on 302893 raw words (563279 effective words) took 8.1s, 69616 effective words/s\n",
      "INFO : EPOCH 8 - PROGRESS: at 3.30% examples, 16826 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 8 - PROGRESS: at 16.51% examples, 42485 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 8 - PROGRESS: at 29.71% examples, 51548 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 8 - PROGRESS: at 42.92% examples, 54881 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 8 - PROGRESS: at 56.13% examples, 56559 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 8 - PROGRESS: at 69.33% examples, 56757 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 8 - PROGRESS: at 82.54% examples, 57521 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 8 : training on 302893 raw words (562948 effective words) took 8.7s, 64632 effective words/s\n",
      "INFO : EPOCH 9 - PROGRESS: at 3.30% examples, 16416 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 9 - PROGRESS: at 16.51% examples, 43098 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 9 - PROGRESS: at 29.71% examples, 51046 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 9 - PROGRESS: at 42.92% examples, 54923 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 9 - PROGRESS: at 56.13% examples, 55587 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 9 - PROGRESS: at 69.33% examples, 55810 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 9 - PROGRESS: at 82.54% examples, 57523 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 9 : training on 302893 raw words (563039 effective words) took 8.8s, 63894 effective words/s\n",
      "INFO : EPOCH 10 - PROGRESS: at 3.30% examples, 13792 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 10 - PROGRESS: at 16.51% examples, 36333 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 10 - PROGRESS: at 29.71% examples, 44162 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 10 - PROGRESS: at 42.92% examples, 49302 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 10 - PROGRESS: at 56.13% examples, 52055 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : EPOCH 10 - PROGRESS: at 69.33% examples, 52259 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : EPOCH 10 - PROGRESS: at 82.54% examples, 52600 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 10 : training on 302893 raw words (563476 effective words) took 9.6s, 58907 effective words/s\n",
      "INFO : training on a 3028930 raw words (5631538 effective words) took 85.4s, 65945 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(all_docs,\n",
    "                                      vector_size=hp[\"size\"],\n",
    "                                      window=hp[\"window\"],\n",
    "                                      min_count=hp[\"min_count\"],\n",
    "                                      workers=hp[\"workers\"],\n",
    "                                      iter=hp[\"iter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, need to be able to go through the model and find the closest documents (words in this case with tags)\n",
    "# for each religion around shared words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_words = ['love', 'life', 'freedom', 'peace', 'happiness']\n",
    "ref_docs = [[w] for w in ref_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ot', 0.1109083890914917),\n",
       " ('q', -0.019228678196668625),\n",
       " ('nt', -0.043169885873794556),\n",
       " ('s', -0.08049650490283966)]"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar(positive=[model.infer_vector(ref_words[4])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
